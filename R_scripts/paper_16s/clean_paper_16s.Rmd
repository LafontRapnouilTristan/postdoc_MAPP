# Intro

The aim of this paper is to investigate microbial communities from northern Peatlands by mean of 16s Metabarcoding.
This investigation is both Taxonomic and Functional.

For about 200 hundred samples from the northern hemisphere we have:

- *Communities info*:  
    * Metabarcoding (16s - bacteria/archae)  
    * Bacteria and photoautotrophs abundances (cytometry)  
    * Enzyme activity (C, N and P cycles; 4 enzymes)  
- *Site bioclimatic data*:  
    * Coordinates  
    * Google Earth Engine (GEE) data  
    

## Metabarcoding

Raw sequencing output (ILLUMINA) were uploaded to Galaxy Toulouse server to be processed using FROGS 4.1 bioinformatic pipeline available on the platform.

### FROGS pipeline

#### I - Preprocess

**Sequencer**: ILLUMINA  
**Input type**: .TAR archive  
**File**: .TAR archive with \*R1.fastq.gz and \*R2.fastq.gz for each sample  
**Merged reads**: NO  
*Read 1 &2 size*: 250  
**Mismatch rate**: 0.1  
**Merge software**: Vsearch  
**Keep unmerged**: NO  
**Amplicon size**: min 200 - max 490  
**Primers**: YES  
**5'**: Forward primer sequence (GTGYCAGCMGCCGCGGT)  
**3'**: Reversed & complemented (ACTYAAAKGAATTGRCGGGG) (online: https://reverse-complement.com/)  

#### II - Clustering Swarm

With fastidious method & d=1 : effectively ASVs

**Aggregation distance**: 1  
**Refine clustering**: YES


#### III - Remove chimera


#### IV - Cluster Filtering

**Minimum prevalence**: NA  
**Minimum proportion/count**: 2  -- Remove rare (proportion) or ASVs with less than X sequences (X=2 => remove singletons)  
**Search contaminant**: On server PhiX database

#### V - Taxonomic Affiliation

Chose ref db (SILVA or µgreen)
Ask for RDP classifier too!


**Blast metrics**:

- **Query Coverage**: Percent of the query sequence length that is included in alignments against the sequence match.

- **E-value**: Indicates the number of hits or alignments that are expected to be seen by random chance with the same score or better. 
The lower the E-value, the more significant the alignment (the closer to 0, the better).
E-value is the default metric used to sort the Descriptions table. 

- **Percent Identity**: Percent of nucleotides or amino acids that are identical between the aligned query and database sequences. 
A query sequence can share low percent identity with a sequence and still be a significant hit. 
It is essential to take the E-value into account and look for similarity between conserved regions (this will be more evident at the amino acid level).


#### VI - Postprocess

NOT filtering assignation within FROGS
Produce assignation stats
Convert .BIOM to .TSV and give a sequence file to keep sequence info!! (More universal format even tho .BIOM is loadable into R for further analysis)

### FROGS Picrust2

[Picrust 2](https://huttenhower.sph.harvard.edu/picrust/) or Phylogenetic Investigation of Communities by Reconstruction of Unobserved States is a software for predicting functional abundances based only on marker gene sequences.

It allows for functional characterization of your microbial communities from 16s metabarcoding data.
We ran Picrust 2 from within the Galaxy Toulouse platform with their pipeline names FROGSFUNC.
The process is divided in three steps that we'll describe briefly but first we aggregate ASVs based on their similarity ,shared taxonomy and sequence coverage.
This step is named FROGS affiliation postprocess on Galaxy Toulouse.

#### Affiliation Postprocess

**Input**: FASTA file containing our sequence and BIOM file with ASVs abundance and taxonomy.  
**Is this amplicon hyper variable length**: NO
**Minimum Identity**: ASVs will be aggregated if they share the same taxonomy AND with at least X% identity. We chose X = 99.  
**Minimum Coverage**: ASVs will be aggregated if they share the same taxonomy AND with at least X% alignment coverage. We chose X = 99.  

#### I - Place sequence and copy numbers

It is the first step of PICRUSt2.
It inserts your study sequences into a reference tree.
By default, this reference tree is based on 20,000 16S sequences from genomes in the Integrated Microbial Genomes database.
The script performs this step, which specifically:
    * Aligns your study sequences with a multiple-sequence alignment of reference 16S, ITS or 18S sequences with HMMER.
    * Finds the most likely placements of your study sequences in the reference tree with EPA_NG or SEPP.
    * Produces a treefile with the most likely placement for each sequence as the new tips with GAPPA.
    * Predicts marker copy number based solely on the sequences of marker genes with PICRUSt2. The available marker genes are 16S, ITS and 18S.


**Input**: FASTA sequence file and BIOM file with ASVs abundance and taxonomy
**Taxonomy marker**: 16s  
**Placement Tool**: epa-ng
**Minimum alignment length**: 0.8 - Proportion of total length of an iput sequence that must align with reference sequences.
All other will be out.

#### II - Calculate Function abundance

It is the second step of PICRUSt2. It ables to predicts :
    * Functional abundances based solely on the sequences of marker genes with PICRUSt2. The available marker genes are 16S, ITS and 18S.
    * Functions, weighted by the relative abundance of ASVs in the community. Inferring the metagenomes of the communities with PICRUSt2.

There are three steps performed at this stage:
    * It runs hidden-state prediction (hsp) to predict function abundances with castor-R of each ASVs placed in the PICRUSt2 reference phylogenetic tree (FROGSFUNC_1_placeseqs_copynumber outputs).
    * The read depth per ASV is divided by the predicted marker (16S/ITS/18S) copy numbers. This is performed to help control for variation in marker copy numbers across organisms, which can result in interpretation issues. For instance, imagine an organism with five identical copies of the 16S gene that is at the same absolute abundance as an organism with one 16S gene. The ASV corresponding to the first organism would erroneously be inferred to be at higher relative abundance simply because this organism had more copies of the 16S gene.
    * The ASV read depths per sample (after normalizing by marker (16S/ITS/18S) copy number) are multiplied by the predicted function copy numbers per ASV.

**Input**: BIOM and FASTA files + TREE file (.nwk) and the MARKER file (.tsv) containing the copy number of the marker produced by the previous step.  
**Taxonomic marker**: 16s  
**Target function database**: Select all  
**NTSI cut-off**: 2 - Any sequence with NTSI > 2 is discarded. Nearest Sequenced Taxon Index (NSTI) is the phylogenetic distance between the ASV and the nearest sequenced reference genome. 
**Identity alignment cut_off**: 0 - Any sequence below this identity threshold agaisnt reference will be discarded.
**Coverage cut-off**: 0 - Same with coverage.
**HSP method**: mp - Hidden state prediction method used. We stick with the maximum parsimony default.

#### III - Calculate Pathways abundance

It is the last step of PICRUSt2. This script infers MetaCyc/KEGG pathway abundances based on EC or KO number abundances.
    * Regroups EC or KO numbers to MetaCyc or KEGG reactions, depending of the unstrat abundances input file.
    * Infers which MetaCyc or KEGG pathways are present based on these reactions with MinPath.
    * Calculates and returns the abundance of pathways identified as present.

Run twice, once for MetaCyc and once for KEGG.
 
 **Input**: TSV file with function abundance prediction from step II (EC - metacyc and KO - kegg).
 **Taxonomic marker**: 16s
 **Pathway reference**: MetaCyc and then Kegg according to the input file (EC - metacyc and KO - kegg).


# Load Packages & custom function

```{r}
EcophyCofog::Library(c("EcophyCofog","metabaR","readr","dplyr","magrittr","ggplot2","reshape2","patchwork","ggpubr","tidyr","forcats","phyloseq","cowplot","readxl","microeco","file2meco","RColorBrewer","vegan","sf","rnaturalearth","rnaturalearthdata"))
```

```{r}
arrowMul <- function(arrows, data, at = c(0, 0), fill = 0.75) {
  u <- c(range(data[,1], range(data[,2])))
  u <- u - rep(at, each = 2)
  r <- c(range(arrows[, 1], na.rm = TRUE), range(arrows[, 2], na.rm = TRUE))
  rev <- sign(diff(u))[-2]
  if (rev[1] < 0)
    u[1:2] <- u[2:1]
  if (rev[2] < 0)
    u[3:4] <- u[4:3]
  u <- u/r
  u <- u[is.finite(u) & u > 0]
  fill * min(u)
}
```

# MetabaR Processing

## prepare metabarlist

```{r}
data_temp <- read_delim(file = "../../data/paper_16s/tsv_com_with_singletons.tsv", # read the tsv file
                        delim = "\t", escape_double = FALSE, 
                        trim_ws = TRUE)

reads_frame <- t(as.matrix(data_temp[,colnames(data_temp)[grepl("L001",colnames(data_temp))]])) # extract site per asvs reads count matrix
colnames(reads_frame) <- data_temp$observation_name #set colnames as cluster ID (asv 'names')


motus_frame <- data_temp[,colnames(data_temp)[!grepl("L001",colnames(data_temp))]] # extract asv information (taxonomy, counts etc)


motus_frame %<>% dplyr::rename(sequence=seed_sequence) # rename seed_sequence columns to sequence
motus_frame %<>% mutate(rdp_tax_and_bootstrap=gsub("\\(|\\)|;$","",rdp_tax_and_bootstrap,perl=T))

motus_frame %<>% mutate(blast_taxonomy=ifelse(blast_taxonomy=="no data","k__;p__;c__;o__;f__;g__;s__",blast_taxonomy))
motus_frame %<>% separate(rdp_tax_and_bootstrap,
                          into=c("Kingdom","boots_Kingdom",
                                 "Phylum","boots_Phylum",
                                 "Class","boots_Class",
                                 "Order","boots_Order",
                                 "Family","boots_Family",
                                 "Genus","boots_Genus",
                                 "Species","boots_Species"),
                          sep=";",
                          remove = F)%>%
    separate(blast_taxonomy,
             into=c("Kingdom_blast",
                    "Phylum_blast",
                    "Class_blast",
                    "Order_blast",
                    "Family_blast",
                    "Genus_blast",
                    "Species_blast"),
             sep=";",
             remove = F)%>%
    mutate(across(.cols=which(grepl("boots_",colnames(.))),.fns=as.numeric))

rownames(motus_frame) <- motus_frame$observation_name # use asv names as rownames of the df
pcr_frame <- data.frame(sample_id=colnames(data_temp)[grep("L001",colnames(data_temp))], type="sample",control_type=NA) # set pcr frame as required by metabaR
rownames(pcr_frame) <- pcr_frame$sample_id # set rownames as site/sample id

sample_frame <- data.frame(sample_id=colnames(data_temp)[grep("L001",colnames(data_temp))]) # create sample frame
rownames(sample_frame) <- sample_frame$sample_id # set rownames as site/sample id

metab_list <- metabarlist_generator(reads = reads_frame, # create metabarlist
                                    motus = motus_frame,
                                    pcrs = pcr_frame,
                                    samples = sample_frame)

rm(data_temp,motus_frame,pcr_frame,reads_frame,sample_frame) # remove temporary files
```



## Start MetabaR processing


### Evaluate PCRs on sequencing depths

Plot number of ASVs and reads per PCRs as well as their correlation.
Correlation between reads and richness are problematic as they might indicate insufficient sequencing depth.
```{r}
data_temp <- metab_list# get one metabarlist

# Compute the number of reads per pcr
data_temp$pcrs$nb_reads <- rowSums(data_temp$reads)

# Compute the number of asvs per pcr
data_temp$pcrs$nb_asvs <- rowSums(data_temp$reads>0)
metab_list <- data_temp
check1 <- reshape2::melt(data_temp$pcrs[,c("control_type", "nb_reads", "nb_asvs")])

p1 <- ggplot(data <- check1, aes(x=control_type, y=value, color=control_type)) + 
    geom_boxplot() + 
    theme_bw() + 
    geom_jitter(alpha=0.2) + 
    scale_color_manual(values = c("brown", "red", "cyan4","pink"), na.value = "darkgrey") +
    facet_wrap(~variable, scales = "free_y") +
    ggtitle("16s")+ 
    theme(axis.text.x = element_text(angle=45, h=1),
          plot.title = element_text(face="bold"))

p2 <-  ggplot(data_temp$pcrs, aes(x=nb_reads, y=nb_asvs, color = control_type)) + 
    geom_point() + 
    theme_bw() + 
    scale_y_log10() + 
    scale_x_log10() + 
    scale_color_manual(values = c("brown", "red", "cyan4","pink"), na.value = "darkgrey")

diagplot <- p1 + p2 + plot_layout(guides="collect") # sample 124 low reads?
rm(p1,p2,data_temp,check1,data)
diagplot
```

```{r}
ggsave("Figures/reads_and_asvs_raw.png",diagplot,width = 14)
```


All samples but one have more than 10000 reads. 
Remove this outlier

Flag pcrs according to seq depths
```{r}
data_temp <- metab_list

# Tag as ok pcrs with more than 10 000 reads
data_temp$pcrs$seqdepth_ok <- ifelse(data_temp$pcrs$nb_reads < 10e3, F, T)

# Overwrite
metab_list <- data_temp 

# proportion of innaceptable pcrs seq depth, control excluded!!
seq_depth_tab <- table(data_temp$pcrs$seqdepth_ok[data_temp$pcrs$type=="sample"]) /
    nrow(data_temp$pcrs[data_temp$pcrs$type=="sample",])
print(seq_depth_tab)
rm(data_temp)
```


### Remove non target ASVs

Tag non target ASVs
```{r}
non_target_prop <- NULL
non_targetconta_prop <- NULL

data_temp <- metab_list

#Flag ASVs corresponding to target (TRUE) vs. non-target (FALSE) taxa 
data_temp$motus$target_taxon <- grepl("Bacteria|Archaea", data_temp$motus$Kingdom)&!grepl("Chloroplast",data_temp$motus$Order)&!grepl("Mitochondria",data_temp$motus$Family)  # MITO and rickettsiales
non_targ_asvs <- data_temp$motus$observation_name[which(data_temp$motus$target_taxon==F)]
# Proportion of each of these over total number of ASVs
non_target_prop <- table(data_temp$motus$target_taxon) / nrow(data_temp$motus)
print(non_target_prop)
# Intersection with extraction contaminant flags (not contaminant = T)
non_target_prop <- table(data_temp$motus$target_taxon)
print(non_target_prop)

# Overwrite
metab_list <- data_temp
```


### Filter and format according to Assignment quality


Tag as poorly assigned Sequences with no assignment at the kingdom lvl (ie with RDP bootstrap values <70: boots_thshld choosen earlier) 
```{r}
boots_thshld <- 0.7 # define a RDP classifier bootstrap value under which we consider affiliation uncertain

# test <- NULL
data_temp <- metab_list
melty <- melt(data_temp$motus, id = c(colnames(data_temp$motus)[which(grepl("boots_",colnames(data_temp$motus))==F)] ))
melty$variable <- gsub("boots_","",melty$variable)
melty$variable <- as.factor(melty$variable)

melty$variable <- fct_relevel(melty$variable,"Kingdom","Phylum","Class","Order","Family","Genus","Species")


plot_tax_boots <-  ggplot(melty,aes(x=variable,y=as.numeric(value))) +
    geom_boxplot(aes(fill=variable))+
    theme_bw()+
    ggtitle("16s")+
    labs(y="conf %")+
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.title.x = element_blank())+
    theme(legend.position = "n",
          plot.title = element_text(face = "bold"))

# Create a "confident taxonomy" (boots >= boots_thshld)
colnames_tmp <- c(gsub("boots_","",grep("boots_",colnames(data_temp$motus), value = T)))

for(j in 1:length(colnames_tmp)){
    
    col <- match(colnames_tmp[j],colnames(data_temp$motus))
    a <- data_temp$motus[,c(col,col+1)]
    tax_lvl <- paste0(tolower(substr(names(a)[2],7,7)),"__")
    conf <- a %>% mutate(col_temp=ifelse(.[[2]]<boots_thshld,gsub(".*",tax_lvl,.[[1]],perl=T),.[[1]]))
    data_temp$motus <- cbind(data_temp$motus,conf[,3])
    colnames(data_temp$motus)[ncol(data_temp$motus)]<- paste0(colnames_tmp[j],"_conf_rdp")
}

# Tag sequences not assigned below Kingdom
data_temp$motus %<>% mutate(good_assign = ifelse(Phylum_conf_rdp=="p__",F,T))

# Overwrite
metab_list <- data_temp
rm(data_temp,melty)

plot_tax_boots
```

### Display noise in ASVs

summarize noise in ASVs
```{r}
# color_scale (create a common scale across datasets)
a <- unlist(combn(c("untargeted_taxon","poorly_assigned"),1,simplify = F))
b <- c("poorly_assigned|untargeted_taxon")
myColors <- RColorBrewer::brewer.pal(8,"Set1")
names(myColors) <- levels(as.factor(c(a,b,"not_artefactual")))
colScale <- scale_fill_manual(name = "grp",values = myColors, drop=F)

data_temp <- metab_list

# Create a table of ASVs quality criteria 
# noise is identified as FALSE in data_temp, the "!" transforms it to TRUE 
asvs.qual <- !data_temp$motus[,c("target_taxon","good_assign")] #not_degraded 
colnames(asvs.qual) <- c("untargeted_taxon","poorly_assigned") #degraded_seq  

asv_noise_tab <- NULL
# Proportion of asvs potentially artifactual (TRUE) based on the criteria used
asv_noise_tab$motus <- prop.table(table(apply(asvs.qual, 1, sum) > 0))

# Corresponding proportion of artifactual reads (TRUE)
asv_noise_tab$reads <- prop.table(xtabs(data_temp$motus$observation_sum~apply(asvs.qual, 1, sum) > 0))

# Proportion of asvs and reads potentially artifactual for each criterion
apply(asvs.qual, 2, sum) / nrow(asvs.qual)
apply(asvs.qual, 2, function(x) sum(data_temp$motus$observation_sum[x])/sum(data_temp$motus$observation_sum))

tmp.asvs <- 
    apply(sapply(1:ncol(asvs.qual), function(x) {
        ifelse(asvs.qual[,x]==T, colnames(asvs.qual)[x], NA)}), 1, function(x) {
            paste(sort(unique(x)), collapse = "|")
        })
tmp.asvs <- as.data.frame(gsub("^$", "not_artefactual", tmp.asvs))
colnames(tmp.asvs) <-  "artefact_type"
tmp.asvs %<>% mutate(artefact_type=as.factor(artefact_type)  )

plot_noise_asv <- ggplot(tmp.asvs, aes(x=1, fill=artefact_type)) +
    geom_bar() +  xlim(0, 2) +
    labs(fill="Artifact type") + 
    coord_polar(theta="y") + theme_void() + 
    scale_fill_manual(name = "grp",values = myColors, drop=F) + 
    theme(legend.direction = "vertical") + 
    ggtitle("16s - ASVS noise")+
    theme(plot.title = element_text(face="bold"))

rm(tmp.asvs,asvs.qual,data_temp,myColors,colScale,a,b)

plot_noise_asv
```

```{r}
ggsave("Figures/noise_in_asvs.png",plot_noise_asv)
```


### Filter 


```{r}
# Use tag-jump corrected metabarlist with the threshold identified above
tmp <- metab_list

# Subset onASVs: we keep asvs that are defined as TRUE following the 
# criteria below (sum of x TRUE is equal to x with the rowSums function)
row.names(tmp$motus) <- colnames(tmp$reads)

tmp <- subset_metabarlist(tmp, "motus", 
                          indices = rowSums(tmp$motus[,c("good_assign", "target_taxon")]) == 2)# remove untargeted taxa and poorly assigned (no assignation @phylum)

# Subset on pcrs and keep only samples 
data_temp <- subset_metabarlist(tmp, "pcrs", 
                                indices = tmp$pcrs[,c("seqdepth_ok")] == TRUE & #, "replicating_pcr" 
                                    tmp$pcrs$type == "sample")

#update counts and reads.    
data_temp$motus$counts = colSums(data_temp$reads)
data_temp$pcrs$reads_post = rowSums(data_temp$reads)
data_temp$pcrs$asvs_post = rowSums(ifelse(data_temp$reads>0, T, F))

check <- melt(data_temp$pcrs[,c("nb_reads", "reads_post", 
                                "nb_asvs", "asvs_post")])
check$type <- ifelse(grepl("asvs", check$variable), "richness", "abundance")

summary_list <- summary_metabarlist(data_temp)

if(sum(colSums(data_temp$reads)==0)>0){print(paste0(folder_names[[i]]," empty asvs present"))}
if(sum(colSums(data_temp$reads)==0)>0){print(paste0(folder_names[[i]]," empty pcrs present"))}

sumpipeline_plots <- ggplot(data = check, aes(x = variable, y = value)) +
    geom_boxplot( color = "darkgrey") +
    geom_jitter(alpha=0.1, color = "darkgrey") +
    theme_bw() +
    facet_wrap(~type, scales = "free", ncol = 5) +
    theme(axis.text.x = element_text(angle=45, h=1),
          axis.title = element_blank()) +
    ggtitle("Effect of cleaning")

cleaned_metablist <- data_temp
rm(tmp,data_temp,check)
sumpipeline_plots
```

```{r}
ggsave("Figures/effect_of_cleaning_asvs_reads.png",sumpipeline_plots,width=14)
```

### Check effect of filter on reads and richness


```{r}
# Using the nb_reads and nb_asvs defined previously in the data_temp$pcrs table
data_temp <- cleaned_metablist

check1 <- reshape2::melt(data_temp$pcrs[,c("control_type", "reads_post", "asvs_post")])

p1 <- ggplot(data <- check1, aes(x=control_type, y=value, color=control_type)) + 
    geom_boxplot() + 
    theme_bw() + 
    geom_jitter(alpha=0.2) + 
    scale_color_manual(values = c("brown", "red", "cyan4","pink"), na.value = "darkgrey") +
    facet_wrap(~variable, scales = "free_y") +
    ggtitle("")+ 
    theme(axis.text.x = element_text(angle=45, h=1),
          plot.title = element_text(face="bold"))

p2 <-  ggplot(data_temp$pcrs, aes(x=reads_post, y=asvs_post, color = control_type)) + 
    geom_point() + 
    theme_bw() + 
    scale_y_log10() + 
    scale_x_log10() + 
    scale_color_manual(values = c("brown", "red", "cyan4","pink"), na.value = "darkgrey")+
    geom_smooth(method="lm",color="darkorange")

diag_plots_post <-  p1 + p2 + plot_layout(guides="collect")
rm(p1,p2,data_temp,check1,data)

diag_plots_post
```


```{r}
ggsave("Figures/cor_asvs_reads_postcleaning.png",diag_plots_post,width=14)
```


OK we have strong correlation between richness and reads within samples (deseq2 or rarefaction?)
Will depend on questions (test link between bioclimatic/geographical variables and the number of reads)
Differential abundance implementation in this models.

## Data wrangling

### Transform to physeq lists


```{r}
data_temp_cleaned <- cleaned_metablist
data_temp <- metab_list

rownames(data_temp$motus) <- colnames(data_temp$reads)
# Get row data only for samples
tmp <- subset_metabarlist(data_temp, table = "pcrs",
                          indices = data_temp$pcrs$type == "sample")

tmpcl <- subset_metabarlist(data_temp_cleaned, table = "pcrs",
                            indices = data_temp_cleaned$pcrs$type == "sample")

# Format for phyloseq

otumat <- as.matrix(tmp$reads)
taxmat <- as.matrix(tmp$motus)
sammat <- left_join(tmp$samples,tmp$pcrs)
OTU <- otu_table(otumat, taxa_are_rows = F)
TAX <- tax_table(taxmat)
SAM <- sample_data(sammat)
rownames(SAM) <- SAM$sample_id
physeq <- phyloseq(OTU,TAX,SAM)
physeq

otumatcl <- as.matrix(tmpcl$reads)
taxmatcl <- as.matrix(tmpcl$motus)
sammatcl <- left_join(tmpcl$samples,tmpcl$pcrs)
OTUcl <- otu_table(otumatcl, taxa_are_rows = F)
TAXcl <- tax_table(taxmatcl)
SAMcl <- sample_data(sammatcl)
rownames(SAMcl) <- SAMcl$sample_id
physeqcl <- phyloseq(OTUcl,TAXcl,SAMcl)
```


```{r}
toremove <- grep("^metab|^cleaned|^physeq|non_targ_asvs|arrowMul", ls(), invert=T, value=T)
rm(list = c(toremove, "toremove"))
gc()
```


remove unwanted samples (Vincent treatments and chile sample)


```{r}
data_temp <- physeqcl
data_temp <- subset_samples(data_temp, !grepl("23[1-7]|239|240|241|042-047|047-042|246b",data_temp@sam_data$sample_id))
data_temp@sam_data$reads_post <- rowSums(data_temp@otu_table)

data_temp@sam_data$reads_post = rowSums(data_temp@otu_table) # reads per samples
data_temp@sam_data$asvs_post = rowSums(ifelse(data_temp@otu_table>0, T, F)) # motus per samples
physeqcl <- data_temp
```


### Transform to microeco

Load site metadata
```{r}
site_coordinates <- read.csv("../../ressource/Site_edaphic_data/site_coord_MAPP.csv")  # get site coordinates
site_coordinates %<>% 
    mutate(Sample=ifelse(nchar(Sample)<3,paste0("0",Sample),Sample))%>% # reformat 1 into 001 and 64 to 064 etc...
    mutate(Sample=ifelse(nchar(Sample)<3,paste0("0",Sample),Sample))
site_coordinates %<>% 
    mutate(Sample=ifelse(grepl("^8",Sample),paste0("0",Sample),Sample)) # fix 87a and 87b to 087a and 087b
```


```{r}
meco_16s <- phyloseq2meco(physeqcl)

meco_16s$sample_table %<>% mutate(Sample = gsub("[A-Z]|-|(?=_).*","",sample_id,perl = T))

# check for diff between sites coordinates obtained form vicent and samples remaining in the final analysis
setdiff(site_coordinates$Sample,meco_16s$sample_table$Sample) # we willingly removed 231-7|239|240|241 / 042 and 047 are mixed up what about : 115/124/199/200/201/205/206/208/209/227/252/253/258

# add coordinates to the three datasets
meco_16s$sample_table %<>% left_join(site_coordinates)

# reset rownames
rownames(meco_16s$sample_table) <- meco_16s$sample_table$sample_id

meco_16s$tidy_dataset() #tidy datasets

# Compute the number of reads per pcr
meco_16s$sample_table$nb_reads <- colSums(meco_16s$otu_table)

# Compute the number of asvs per pcr
meco_16s$sample_table$nb_reads <- colSums(meco_16s$otu_table>0)

gc()
```

# Explore 16s communities

Samples 256 and 87 are duplicated (a and b). 
256b and 87a are retained because of their higher sequencing depth.
```{r}
colSums(meco_16s$otu_table[,grepl("256",colnames(meco_16s$otu_table))]);colSums(meco_16s$otu_table[,grepl("87",colnames(meco_16s$otu_table))])
```

Moreover we lack geographical info about samples 116, 119 and 166 and samples 042 and 047 have been mixed and can't be exploited.

```{r}
meco_16s$otu_table <- meco_16s$otu_table[,!grepl("256a|87b|116|119|166",colnames(meco_16s$otu_table))] # remove these samples
meco_16s$filter_pollution(taxa = c("mitochondria", "chloroplast")) # remove chloroplast and mitochondria if some passed the previous filter
meco_16s$tidy_dataset() # tidy the microeco object
```

## Investigate singletons

Visualize number of singletons
```{r}
data_temp <- meco_16s # get data

n_single_asv <- sum(rowSums(data_temp$otu_table)==1) # number of single asvs (those that are present exactly once across all sites)
n_non_single_asv <- sum(rowSums(data_temp$otu_table)>1) # number of non single asvs
n_single_reads <- sum(data_temp$otu_table[which(rowSums(data_temp$otu_table)==1),]) # number of reads from the data set that correspond to asv singletons (sum of reads on singletons subset)
n_non_single_reads <- sum(data_temp$otu_table[which(rowSums(data_temp$otu_table)>1),]) # same but for non singletons
df_tmp <- data.frame(asv_val=c(n_single_asv,n_non_single_asv),
                     read_val=c(n_single_reads,n_non_single_reads),
                     type=c("n_single","n_non_single"))
df_tmp %<>% mutate(pct_asvs=asv_val/sum(asv_val), # convert to %
                   pct_reads=read_val/sum(data_temp$otu_table))%>%
    reshape2::melt()%>%
    filter(grepl("pct",variable))

singletons <- ggplot(df_tmp,aes(x = variable,y = value, fill=type))+
    geom_bar(position="stack",stat = "identity")+
    theme_classic2()+
    scale_fill_manual(values = c("lightseagreen","mediumvioletred"))+
    theme(plot.title = element_text(face='bold'))+
    ylab("% of singletons")+
    xlab("ASVs vs Reads")+
    scale_y_continuous(expand = c(0,0))


alpha <- colSums(data_temp$otu_table!=0) # sample alpha diversity as the number of non null rows (asvs)
single_asv <-  colSums(filter(data_temp$otu_table,rowSums(data_temp$otu_table)==1)==1) # number of singletons asvs within sample as the number of asvs restricted to this sample (subset rowsums) and with one read (==1)
# non_single_asv2 <- colSums(data_temp$otu_table>1)
non_single_asv <- alpha-single_asv # ASVs that might be singletons within the sample but not the dataset 
df_tmp <- data.frame(value = c(rbind(single_asv,non_single_asv)),
                     type=c("n_single","n_non_single"),
                     sample=rep(1:ncol(data_temp$otu_table),each=2))%>%
    group_by(sample)%>%
    mutate(val_pct= value/sum(value))

alpha_plot <- ggplot(df_tmp,aes(x=sample,y=value,fill=type))+
    geom_bar(stat="identity",position = "stack",width = 1)+
    theme_classic2()+
    ggtitle("16s singletons repartition")+
    theme(plot.title = element_text(face='bold'))+
    scale_y_continuous(expand = c(0,0))+
    scale_x_continuous(expand = c(0,0))+
    xlab("samples")+
    ylab("Richness")+
    scale_fill_manual(values=c("lightseagreen","mediumvioletred"))

alpha_plot_pct <- ggplot(df_tmp,aes(x=sample,y=val_pct,fill=type))+
    geom_bar(stat="identity",position = "stack",width = 1)+
    theme_classic2()+
    theme(plot.title = element_text(face='bold'))+
    scale_y_continuous(expand = c(0,0))+
    scale_x_continuous(expand = c(0,0))+
    xlab("samples")+
    ylab("Richness")+
    scale_fill_manual(values=c("lightseagreen","mediumvioletred"))


meco_16s_singl <- clone(meco_16s)
meco_16s_singl$otu_table %<>% filter(rowSums(.)==1)
meco_16s_singl$tidy_dataset()
meco_16s_singl$sample_table %<>% mutate(dumb_group = "sample")
meco_16s_singl$cal_abund()
meco_16s_singl <- trans_abund$new(meco_16s_singl,taxrank = "Order_conf_rdp",ntaxa = 8,groupmean = "dumb_group")
    
plot_data <- meco_16s_singl$data_abund
use_taxanames <- c(meco_16s_singl$data_taxanames,"unidentified")
plot_data$Taxonomy[!plot_data$Taxonomy %in% use_taxanames] <- "Others"
plot_data %<>% 
    dplyr::group_by(!!!syms(c("Taxonomy","Sample"))) %>% 
    dplyr::summarise(Abundance = sum(Abundance)) %>% 
    as.data.frame(stringsAsFactors = FALSE)
plot_data$Taxonomy %<>% factor(., levels = c(use_taxanames[-9], "Others", "unidentified"))
plot_data$label <- paste0(round(plot_data$Abundance, 1),"%")
donut_comp <- ggdonutchart(plot_data,"Abundance",
             fill="Taxonomy",
             label = "label",
             color = "white",
             palette = c(colorRampPalette(brewer.pal(8, "Dark2"))(9),"lightgrey"))+
    theme(legend.position = "right")


singleton_plots <- alpha_plot + theme(legend.position = "n") + alpha_plot_pct + singletons + theme(legend.position = "n") + donut_comp  + plot_layout(widths = c(1, 1))

ggsave("Figures/diagnose_singletons.png",singleton_plots,width = 14,height = 14)
```


We have only singletons in 16s but ~80% of our ASVs but ~4% of our reads.
Can this be interpretated as high endemism in peatlands bacteria??
And loads of rare strains? 
We are working at low resolution with SWARM clustering, increasing singletons.
We use the 'fastidious' option that should prevent to many singletons if they are close https://peerj.com/articles/1420/


Can't remove that many ASVs,huge effect on alpha div no?
Tho quite well distributed across samples.
Should reduce divergence between my samples.

Shall we keep singletons for alpha div and stuff but aggregate at some phylogenetical levels for modeling approaches? 
Hypothesis that phylogeny retains ecological features but is it true for bacteria? 



## Alpha-div

re-compute for each samples the sequencing depth and richness
```{r}
meco_16s$sample_table %<>%
    select(sample_id,nb_reads,nb_asvs,Sample,Y,X) %>%
    mutate(nb_reads=colSums(meco_16s$otu_table),
           nb_asvs=colSums(meco_16s$otu_table>0))
```


same without singletons
```{r}
meco_16s_ns <- clone(meco_16s)
meco_16s_ns$otu_table %<>% filter(rowSums(meco_16s_ns$otu_table)>1)
meco_16s_ns$tidy_dataset()
meco_16s_ns$sample_table %<>%
    mutate(nb_reads=colSums(meco_16s_ns$otu_table),
           nb_asvs=colSums(meco_16s_ns$otu_table>0))
```


```{r}
plot_alpha_divz_sgnsg <- NULL
for(i in c("meco_16s","meco_16s_ns")){
    
    tmp <- get(i)
    plot_title <- ifelse(match(i,c("meco_16s","meco_16s_ns"))==1,"With singletons","Without singletons")
    
    distrib_reads_and_asvs <- tmp$sample_table %>%
        ggplot(aes(x=nb_reads))+
        geom_histogram(bins=30,color='darkorange',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()+
        tmp$sample_table %>%
        ggplot(aes(x=nb_asvs))+
        geom_histogram(bins=30,color='deeppink4',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()
    
    cor_reads_asvs <- tmp$sample_table %>%
        ggplot(aes(x=nb_reads, y=nb_asvs))+
        geom_point(color='darkgrey')+
        scale_y_continuous(expand = expansion(mult = c(0,0.05), add= c(0,0)))+
        scale_x_continuous(expand = expansion(mult = c(0,0.05), add= c(0,0)) )+
        theme_classic2()+
        geom_smooth(method = 'lm',lty=2,color='darkorange')+
        ggpmisc::stat_poly_eq(ggpmisc::use_label(c("eq", "R2", "p")))
    
    tab16s <- t(otu_table(file2meco::meco2phyloseq(tmp))) # get the community
    class(tab16s) <- "matrix" # change class
    curve16s <- rarecurve(tab16s,step=100) # use vegan rarefaction curves
    names(curve16s) <- rownames(tab16s) # name curves after sample IDs
    
    # Coerce data into "long" form.
    protox <- mapply(FUN = function(x, y) {
        mydf <- as.data.frame(x)
        colnames(mydf) <- "value"
        mydf$samples <- y
        mydf$subsample <- attr(x, "Subsample")
        mydf
    }, x = curve16s, y = as.list(names(curve16s)), SIMPLIFY = FALSE)
    
    xy <- do.call(rbind, protox)
    rownames(xy) <- NULL  # pretty
    
    # Plot.
    rare16s <- ggplot(xy, aes(x = subsample, y = value, group = samples )) +
        theme_classic2() +
        scale_color_discrete(guide = "none") +  # turn legend on or off
        geom_line(color = "aquamarine4")+
        xlab("nb reads")+
        ylab("ASVs count")
    
    plot_alpha_divz_sgnsg[[i]] <- distrib_reads_and_asvs/(cor_reads_asvs+rare16s)+ plot_annotation(title = plot_title) & theme(plot.title = element_text(face='bold'))
    
}
plot_alpha_divz_sgnsg[[1]]|(plot_alpha_divz_sgnsg[[2]])
```

```{r}
ggsave("Figures/alpha_div_with_sing.png",plot_alpha_divz_sgnsg[[1]],width = 14,height = 14)
ggsave("Figures/alpha_div_no_sing.png",plot_alpha_divz_sgnsg[[2]],width = 14,height = 14)
```


## Composition

donuts plot without singletons fur 16s
```{r}
plots_comp <- NULL
for( i in c("Order_conf_rdp","Genus_conf_rdp")){
    tmp16s <- clone(meco_16s_ns)
    tmp16s$sample_table %<>% mutate(dumb_group = "sample")
    tmp16s$otu_table %<>% filter(rowSums(.)>1)
    tmp16s$tidy_dataset()
    
    tmp16s$cal_abund()
    tmp16s <- trans_abund$new(tmp16s,taxrank = i,ntaxa = 8,groupmean = "dumb_group")
    
    plot_data <- tmp16s$data_abund
    use_taxanames <- tmp16s$data_taxanames
    plot_data$Taxonomy[!plot_data$Taxonomy %in% c(use_taxanames,"unidentified")] <- "Others"
    plot_data %<>% 
        dplyr::group_by(!!!syms(c("Taxonomy","Sample"))) %>% 
        dplyr::summarise(Abundance = sum(Abundance)) %>% 
        as.data.frame(stringsAsFactors = FALSE)
    plot_data$Taxonomy %<>% factor(., levels = c(use_taxanames[-9], "Others","unidentified"))
    plot_data$label <- paste0(round(plot_data$Abundance, 1),"%")
    donut_comp <- ggdonutchart(plot_data,"Abundance",
                               fill="Taxonomy",
                               label = "label",
                               color = "white",
                               palette =  c(colorRampPalette(brewer.pal(8, "Dark2"))(9),"lightgrey"))+
        theme(legend.position = "right")
    
    tmp16s <- clone(meco_16s_ns)
    tmp16s$sample_table %<>% mutate(dumb_group = "sample")
    tmp16s$otu_table %<>% filter(rowSums(.)>1)
    tmp16s$tidy_dataset()
    
    tmp16s <- trans_abund$new(tmp16s,taxrank = i,ntaxa = 8)
    
    bar_comp <- tmp16s$plot_bar(others_color = '#666666',color_values = colorRampPalette(brewer.pal(8, "Dark2"))(9), xtext_keep = FALSE, legend_text_italic = FALSE)+
        geom_col(width = 1)
    plots_comp[[i]] <- donut_comp + bar_comp + guides(fill = guide_legend(reverse = T)) +  plot_annotation(title = 'No singletons') & theme(plot.title = element_text(face='bold'))
}
plots_comp$Order_conf_rdp
```


```{r}
ggsave("Figures/comp_order.png",plots_comp$Order_conf_rdp,width = 14)
ggsave("Figures/com_genus.png",plots_comp$Genus_conf_rdp,width = 14)
```

```{r}
df <- meco_16s_ns$otu_table
df <- melt(as.matrix(df))
colnames(df) <- c("ASVs","Samples","Reads")
df%>%
    mutate(Samples = gsub("[A-Z]|-|(?=_).*","",Samples,perl = T))%>%
    ggplot(aes(x=ASVs,y=Samples,fill=log(Reads)))+
    geom_tile()
```


Distribution and prev/relab

```{r}
prev <- rowSums(meco_16s_ns$otu_table>0)
a <- prev%>%
    as.data.frame()%>%
ggplot(aes(x=prev))+
        geom_histogram(bins=30,color='darkcyan',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()+
    xlab("Cluster prevalence across samples")+
    scale_y_log10()

median_relab <- NULL
mean_relab <- NULL
for(i in 1:nrow(meco_16s_ns$otu_table)){
    median_relab <- c(median_relab,median(as.numeric(meco_16s_ns$otu_table[i,]/colSums(meco_16s_ns$otu_table)))*100)
    mean_relab <- c(mean_relab,mean(as.numeric(meco_16s_ns$otu_table[i,]/colSums(meco_16s_ns$otu_table)))*100)
}
b <- median_relab%>%
    as.data.frame()%>%
ggplot(aes(x=median_relab))+
        geom_histogram(bins=30,color='burlywood3',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()+
    xlab("Cluster median relative abundance within samples (%)")+
    scale_y_log10()

c <- mean_relab%>%
    as.data.frame()%>%
ggplot(aes(x=mean_relab))+
        geom_histogram(bins=30,color='darkolivegreen4',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()+
    xlab("Cluster mean relative abundance within samples (%)")+
    scale_y_log10()

(a+b)/(c+plot_spacer())
```


```{r}
ordi16s <- ordinate(file2meco::meco2phyloseq(meco_16s_ns), "NMDS", "horn")

tmp16s <- clone(meco_16s_ns)
tmp16s$sample_table %<>% mutate(dumb_group = "sample")
tmp16s$otu_table %<>% filter(rowSums(.)>1)
tmp16s$tidy_dataset()

tmp16s <- trans_abund$new(tmp16s,taxrank = "Genus",ntaxa = 8)

df_ordi_16s  <- merge(ordi16s$species,meco_16s_ns$tax_table,by = 'row.names')%>%
    mutate(Genus=ifelse(Genus%in%paste0("g__",tmp16s$data_taxanames),Genus,"Other"))%>%
    mutate(Genus=gsub('g__','',Genus))%>%
    mutate(Genus=as.factor(Genus))%>%
    mutate(Genus=fct_relevel(Genus,c(tmp16s$data_taxanames,"Other")))

ggplot(df_ordi_16s,aes(x=MDS1,y=MDS2,color=Genus,alpha=Genus))+
    geom_point(shape=16)+
    theme_classic()+
    ylab(paste0("NMDS 2"))+
    xlab(paste0("NMDS 1"))+
    geom_vline(xintercept=0,lty=2)+
    geom_hline(yintercept=0,lty=2)+
    scale_color_manual(values=colorRampPalette(brewer.pal(8, "Dark2"))(9))+
    scale_alpha_manual(values=c(rep(1,8),.5))
```

Still some highly suspicious ASVs.

clean environment and save datasets
```{r}

```


# Sites

We collated data from Google Earth Engine (GEE) using sample coordinate provided by our collaborators.
These data are from different datasets and satellite sources available on  [GEE website](https://developers.google.com/earth-engine/datasets).

Temporal data (Terra Climate, SMAP soil, Vegetation, Productivity) are average over years  and tagged "_lt" for "long-term".
In addition to these long term data we retrieved monthly data from march (_03) to october (_10) of the sampling year (2021).
These will be used to produce "short-term" (_st) data with the hypothesis that microbial communities are strongly affected by recent meteorology.

For each variable is given: name - unit - scale - resolution.


## Available Data

### Terra Climate

These are monthly weather data.
Long term : 2011/01/01 to 2021/01/01

With scale being a scaling factor by which to multiply the variable to get the right values.

**aet**: actual evapotranspiration - mm - 0.1 - 4638.3m
**def**: Climate water deficit - mm - 0.1 - 4638.3m
**pdsi**: Palmer drought severity index (temperature and precipitation -10 to +10 : dry to wet) - no unit - 0.01 - 4638.3m  
**pet**: Reference evapotranspiration - mm - 0.1 - 4638.3m  
**pr**: Precipitation accumulation (@ the end of the month) - mm - no scale - 4638.3m  
**srad**: Downward surface shortwave radiation - W/m^2 - 0.1 - 4638.3m  
**swe**: Snow water equivalent - mm - no scale - 4638.3m  
**tmmn**: Minimum Temperature - °C - 0.1 - 4638.3m  
**tmmx**: Maximum Temperature - °C - 0.1 - 4638.3m  
**vap**: Vapor pressure - KPa - 0.001 - 4638.3m  
**vpd**: Vapor pressure deficit - kPa - 0.01 - 4638.3m  
**moist**: Soil moisture - mm - 0.1 - 4638.3m  

### SMAP soil

NASA_USDA/HSL/SMAP10KM_soil_moisture

These are monthly soil data.
Long term : 2016/01/01 to 2021/01/01

**ssm**: Surface soil moisture
**susm**: Subsurface soil moisture
**smp**: Soil moisture profile
**ssma**: Surface soil moisture anomaly
**susma**: Subsurface soil moisture anomaly

### Topography

USGS/GMTED2010_FULL

**height**: Min elevation - m - no scale - 231.92m

### Vegetation

MODIS/006/MOD13Q1
MODIS/006/MCD15A3H

Long term : 2011/01/01 to 2021/01/01

**ndvi**: Normalized difference vegetation index over 16days - no unit - 0.0001 - 250m
**evi**: Enhanced vegetation index over 16days - no unit - 0.0001 - 250m
**lai**: Leaf area index over 4days - no unit - 0.1 - 500m
**fpar**: Fraction of absorbed photosynthecatilly active radiation (400-700nm) absorbed by the green elements of a vegetation canopy over 4days - no unit - 0.001 - 500m

### Productivity

MODIS/006/MOD17A2H

Long term : 2011/01/01 to 2021/01/01

**gpp**: Gross primary production 8days - kg*C/m^2 - 0.0001 - 500m
**npp**: Net primary production 8days - kg*C/m^2 - 0.0001 - 500m

### Open Land Map Soil

OpenLandMap/SOL/SOL_BULKDENS-FINEEARTH_USDA-4A1H_M/v02
OpenLandMap/SOL/SOL_CLAY-WFRACTION_USDA-3A1A1A_M/v02
OpenLandMap/SOL/SOL_ORGANIC-CARBON_USDA-6A1C_M/v02
OpenLandMap/SOL/SOL_PH-H2O_USDA-4C1A2A_M/v02
OpenLandMap/SOL/SOL_WATERCONTENT-33KPA_USDA-4B1C_M/v01

**bulk**: Bulk density @ 0 cm depth - kg/m^3 - 10 - 250m
**clay**: Clay content @ 0 cm depth - % (kg / kg) - no scale - 250m
**soc**: Organic carbon content @0 cm depth - g/kg - 5 - 250m
**ph**: pH in H20 @ 0 cm depth - pH - 10 - 250m
**water**: Water content @ field capacity (33kPa) @ 0 cm depth - % - no scale _ 250m

### Human variables: Global Human Modification

CSP/HM/GlobalHumanModification

**ghm**: Global human modification index - fraction of a km^2 - no scale - 1000m


## Map Sites

```{r}
world <- ne_countries(scale = "medium", returnclass = "sf")

# Subset the world data to include only the specified latitude range
cropping <- as(raster::extent(-180, 180, 20, 90), "SpatialPolygons")


world_subset <- raster::crop(as_Spatial(world),cropping)
# Create a Lambert azimuthal equal-area projection centered on the North Pole
lambert_proj <- st_crs("+proj=laea +lat_0=90 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs")
world_lambert <- st_transform(st_as_sf(world_subset), crs = lambert_proj)


(sites <- st_as_sf(meco_16s_ns$sample_table, coords = c("X", "Y"), 
                   crs = 4326, agr = "constant"))

# Plot the map
ggplot() +
    geom_sf(data = world_lambert, fill = "grey", color = "darkgrey", size = 0.1)+
    geom_sf(data = sites, size = 3, shape = 23, fill = "darkred",alpha=.7) +
    ggtitle("MAPP 16s sites") +
    theme(plot.title = element_text(hjust = 0.5, size = 16))+
    theme_minimal()+
    theme(panel.background = element_rect(fill = "azure"))
```


## Select sites & deal with NAs

```{r}
site_data_raw <- read.csv("../../ressource/Site_edaphic_data/MappPtsSampled.csv")
site_data_raw %<>%
    mutate(Sample=ifelse(nchar(Sample)<3,paste0("0",Sample),Sample))%>%
    mutate(Sample=ifelse(nchar(Sample)<3,paste0("0",Sample),Sample))%>%
    mutate(Sample=ifelse(grepl("^8",Sample),paste0("0",Sample),Sample))%>% # fix 87a and 87b sample ids
    arrange(Sample)# fix sample ID to match the format of other MAPP datasets

site_data <- site_data_raw %>% filter(Sample%in%meco_16s_ns$sample_table$Sample)%>%
    mutate(across(.cols= colnames(site_data_raw),.fns = ~replace(., . ==  -9999 , NA)))
```


### Count NAs and Imputations


```{r}
site_data %<>% # remove short term variable of wrong months from site data
    select(names(site_data)[!grepl('06|07|08|09|10', names(site_data))])

as.data.frame(colSums(is.na(site_data)))%>% # plot NAs counts and %of variables
    rename(nb_na=`colSums(is.na(site_data))`)%>%
    mutate(var=rownames(.))%>%
    filter(nb_na!=0)%>%
    ggplot(aes(x=var,y=nb_na))+
    geom_bar(stat='identity',fill='midnightblue',width=1)+
    theme_classic2()+
    theme(axis.text.x = element_text(angle=45,hjust=1),
          plot.title = element_text(face="bold"))+
    scale_y_continuous(expand = c(0,0))+
    ggtitle("Number of missing values")+
    as.data.frame(colSums(is.na(site_data)))%>%
    rename(nb_na=`colSums(is.na(site_data))`)%>%
    mutate(var=rownames(.),
           na_pct=(nb_na/202)*100)%>%
    filter(nb_na!=0)%>%
    mutate(non_na_pct=100-na_pct)%>%
    reshape2::melt(id.variables=c(var,nb_na))%>%
    filter(variable!="nb_na")%>%
    mutate(variable=fct_relevel(variable,"non_na_pct","na_pct"))%>%
    ggplot(aes(x=var,y=value,fill=variable))+
    geom_bar(stat='identity',width=1,position="fill")+
    scale_fill_manual(values=c("lightgrey","mediumvioletred"))+
    theme_classic2()+
    theme(axis.text.x = element_text(angle=45,hjust=1),
          plot.title = element_text(face="bold"))+
    ggtitle("Percentage of missing values")+
    scale_y_continuous(expand = c(0,0))
```

### Create short term

```{r}
var_lt <- colnames(site_data)[grep('_lt', names(site_data))] # extract long term variables (as they also exist on monthly basis)
months_we_want <- c("03","04","05") # march/april/may  (1 to 4 months before sampling)
var_st <- colnames(site_data)[grep('03|04|05', names(site_data))] # get columns of monthly variables

site_data_lt <- site_data%>% # subset long term variables
    select(all_of(c("X","Y",var_lt,"bulk","gHM","height","ph","soc","water")))
site_data_st <- site_data%>% # subset chort term variables
    select(all_of(c("X","Y",var_st,"bulk","gHM","height","ph","soc","water")))
```

### Impute missing data

We perform missing data imputation using missMDA package.

```{r} 
# Get the number of PCs we need to infer our missing values
ncp_data_lt <- site_data %>%  
    select(all_of(c("X","Y",var_lt,"bulk","gHM","height","ph","soc","water"))) %>%
    missMDA::estim_ncpPCA(scale=T)

ncp_data_st <- site_data %>% 
    select(all_of(c("X","Y",var_st,var_lt,"bulk","gHM","height","ph","soc","water"))) %>%
    mutate(across(everything(),as.numeric))%>%
    missMDA::estim_ncpPCA(scale=T)

# Impute our missing values and store them in a new 'imputed' data frame
imputed_data_lt <- site_data %>% 
    select(all_of(c("X","Y",var_lt,"bulk","gHM","height","ph","soc","water")))%>%
    missMDA::imputePCA(ncp=5,scale=T)
imputed_data_lt <- imputed_data_lt$completeObs %>% as.data.frame()

imputed_data_st <- site_data %>% 
    select(all_of(c("X","Y",var_st,var_lt,"bulk","gHM","height","ph","soc","water")))%>%
    missMDA::imputePCA(ncp=5,scale=T)
imputed_data_st <- imputed_data_st$completeObs %>% as.data.frame()
```

Our short term data are averaged over three months (march to may).
We need to test if imputed before or on mean changes a lot to imputed values.

```{r}
# Test by averaging short term variables before imputation
site_data_st_before_imputation <- site_data %>% 
    select(all_of(c("X","Y",var_lt,var_st,"bulk","gHM","height","ph","soc","water")))%>%
    mutate(across(.cols=all_of(var_lt),
                  ~get(paste0(gsub("_lt","_",cur_column()), months_we_want[1]))+
                       get(paste0(gsub("_lt","_",cur_column()), months_we_want[2]))+
                       get(paste0(gsub("_lt","_",cur_column()), months_we_want[3]))/3,
                  .names = "{col}__st"
    ))

var_to_remove <- colnames(site_data_st_before_imputation)[grep('_[0-9]{2}', names(site_data_st_before_imputation))]
site_data_st_before_imputation %<>%
    select(-one_of(var_to_remove))%>% 
    rename_with(~ stringr::str_remove(., "_lt_"), everything())

st <- names(site_data_st_before_imputation)[grep('_st', names(site_data_st_before_imputation))]
ncp_data_before_st <- site_data_st_before_imputation %>% 
    select(all_of(c("X","Y",st,"bulk","gHM","height","ph","soc","water"))) %>%
    mutate(across(everything(),as.numeric))%>%
    missMDA::estim_ncpPCA(scale=T)


site_data_st_before_imputation_imputed <- site_data_st_before_imputation %>% 
    select(all_of(c("X","Y",st,"bulk","gHM","height","ph","soc","water")))%>%
    missMDA::imputePCA(ncp=3,scale=T)
site_data_st_before_imputation_imputed <- site_data_st_before_imputation_imputed$completeObs %>% as.data.frame()

# Here we average on imputed variables
site_data_st_after_imputation <- imputed_data_st %>%
    mutate(across(.cols=all_of(var_lt),
                  ~get(paste0(gsub("_lt","_",cur_column()), months_we_want[1]))+
                       get(paste0(gsub("_lt","_",cur_column()), months_we_want[2]))+
                       get(paste0(gsub("_lt","_",cur_column()), months_we_want[3]))/3,
                  .names = "{col}__st"
    ))

var_to_remove <- colnames(site_data_st_after_imputation)[grep('_[0-9]{2}', names(site_data_st_after_imputation))]
site_data_st_after_imputation %<>%
    select(-one_of(var_to_remove))%>% 
    rename_with(~ stringr::str_remove(., "_lt_"), everything())%>%
    select(all_of(c("X","Y",st,"bulk","gHM","height","ph","soc","water")))
```

Test multiple imputation
```{r}
# test <- site_data_st_before_imputation %>% 
#     select(all_of(c("X","Y",st,"bulk","gHM","height","ph","soc","water")))%>%
#     missMDA::MIPCA(ncp=3,scale=T,nboot = 1000)
# 
# plot(test)
```
```{r}
# test2 <- site_data %>% 
#     select(all_of(c("X","Y",var_st,var_lt,"bulk","gHM","height","ph","soc","water")))%>%
#     missMDA::MIPCA(ncp=3,scale=T,nboot = 1000)
# 
# plot(test2)
```

### Histogram of variables


```{r}
data_sets <- c("site_data_lt",
               "site_data_st",
               'imputed_data_lt',
               'imputed_data_st',
               "site_data_st_after_imputation",
               "site_data_st_before_imputation_imputed")

for(j in data_sets){
    message(j)
    tmp <- get(j)
    plot_list <- NULL
    for(i in 1:ncol(tmp)){
        plot_list[[names(tmp[i])]] <- local({
            i <- i
            ggplot(tmp,aes(x=tmp[,i]))+
            geom_histogram(fill="darkolivegreen3")+
            theme_classic2()+
            xlab(names(tmp[i]))})
        
    }
    ggsave(plot=ggarrange(plotlist =  plot_list,
                          ncol = ceiling(sqrt(i)),
                          nrow = ceiling(sqrt(i))),
                          filename=paste0("Figures/histograms/",j,".png"),
                          height =25,
                          width = 25)
}
```


### PCA sites

```{r}
pca_sites_lt <- imputed_data_lt %>% 
    select(!grep('_st', names(imputed_data_lt)))%>%
    FactoMineR::PCA(scale.unit = T,
                    graph = F)

pca_sites_before_st <- site_data_st_before_imputation_imputed %>% 
    select(grep('_st', names(site_data_st_before_imputation_imputed)))%>%
    FactoMineR::PCA(scale.unit = T,
                    graph = F) # NAs

pca_sites_after_st <- site_data_st_after_imputation %>% 
    select(grep('_st', names(site_data_st_after_imputation)))%>%
    FactoMineR::PCA(scale.unit = T,
                    graph = F)
```

```{r}
data_sets <- c("pca_sites_lt","pca_sites_before_st","pca_sites_after_st")
col <- c("darkorange","cornflowerblue","darkorchid4")
pca_site_biplot <- NULL
scree_plot_list <- NULL

for(i in data_sets){
    pca_tmp <- get(i)
    
    eig <- pca_tmp$eig
    coord_pca <- as.data.frame(pca_tmp$ind$coord)
    coord_pca$site <- site_data$Sample
    coord_pca$country <- site_data$Country
    
    mul <- arrowMul(as.data.frame(pca_tmp$var$coord),
                    pca_tmp$ind$coord)
    
    pca_site_biplot[[i]] <- ggplot()+
        geom_point(coord_pca,mapping=aes(x=Dim.1,y=Dim.2),color=col[match(i,data_sets)])+
        theme_classic2()+
        geom_vline(xintercept = 0,lty=2)+
        geom_hline(yintercept = 0,lty=2)+
        # ggrepel::geom_label_repel(data=filter(coord_pca,Dim.2< -3),aes(x=Dim.1,y=Dim.2,label=site))+
        xlab(paste0("PC1 (",round(eig[1,2],2),"%)"))+
        ylab(paste0("PC2 (",round(eig[2,2],2),"%)"))+
        geom_segment(data= as.data.frame(pca_tmp$var$coord),aes(x = 0, y = 0, xend=mul*Dim.1, yend=mul*Dim.2),
                     lineend = "round", 
                     linejoin = "round",
                     linewidth = .75, 
                     arrow = arrow(length = unit(0.2, "inches")),
                     colour = "black" 
        )+
        ggrepel::geom_text_repel(data = data.frame(var=rownames(pca_tmp$var$coord),pca_tmp$var$coord*mul), # add variable names at the end of arrows
                                 aes(x = ifelse(Dim.1<0,Dim.1*1.1,Dim.1*1.1), # nudge a bit the coordinates so that they're not on the arrows
                                     y = ifelse(Dim.2<0,Dim.2*1.1,Dim.2*1.1),
                                     label = var),
                                 max.overlaps = getOption("ggrepel.max.overlaps", default = 15))+
        ggtitle(paste0("Biplot sites ",i))+
        theme(plot.title = element_text(face="bold"))
    
    scree_plot_list[[i]] <- factoextra::fviz_screeplot(pca_tmp,barfill = col[match(i,data_sets)], barcolor=col[match(i,data_sets)])+
        ggtitle(paste0("Biplot sites ",i))+
        theme(plot.title = element_text(face="bold"))
}
```

```{r}
(pca_site_biplot$pca_sites_lt+
    pca_site_biplot$pca_sites_before_st+
    pca_site_biplot$pca_sites_after_st)/(
    scree_plot_list$pca_sites_lt+
    scree_plot_list$pca_sites_before_st+
    scree_plot_list$pca_sites_after_st)
```

## Select variables

aet
pr
swe
tmmn
tmmx # delta tminmax?
vap
vpd
moist
ssm
elevation
evi
fpar
gpp
ghm
ph
soc
water



# Save data sets for Hmsc 

I need meco_16s_ns which is our communities with no singletons.
I need site data that are imputed.
I'll go with data imputed after short term averaging to reduce noise.
Selected variables!!

```{r}
site_data_st_before_imputation_imputed$Sample <- site_data$Sample
imputed_data_lt$Sample <- site_data$Sample
combine<-site_data_st_before_imputation_imputed%>%
  left_join(imputed_data_lt, by="Sample", suffix=c("",".y"))%>%
  select(-ends_with(".y"))%>%
    relocate(Sample)


# save.image(file='outputs/preproc_myenv.RData')
save(meco_16s_ns,file = "outputs/meco_16s.RData")
save(combine,file = "outputs/site_data_imputed.RDS")
save(non_targ_asvs, file = "non_targ_asvs.rds")
rm(list=ls())
gc()
```
# 16s and sites relations

```{r}
var_lt <- colnames(combine)[which(grepl("lt",colnames(combine)))]
var_st <- colnames(combine)[which(grepl("st",colnames(combine)))]
var_interest <- c("aet","pr","swe","tmmn","tmmx","vap","vpd","moist","ssm","elevation","evi","fpar","gpp","ghm","ph","water","soc")
var_lt <- var_lt[which(gsub("_lt","",var_lt)%in%var_interest)]
var_st <- var_st[which(gsub("_st","",var_st)%in%var_interest)]

XFormula_lt <- paste(var_lt,sep="", collapse = "+")
XFormula_st <- paste(var_st,sep="", collapse = "+")

XFormula_lt <- paste0("nb_asvs ~ ph +", XFormula_lt ,"+ nb_reads + (1|Sample_id)")
XFormula_st <- paste0("nb_asvs ~ ph +", XFormula_st ,"+ nb_reads + (1|Sample_id)")

combine %<>% leftjoin(meco_16s_ns$sample_table)

test <- lme4::lmer(formula = XFormula_lt,
                    data = combine)
```


# Picrust2 process


## Alignment and stuff

```{r}
picrust_closestref <-  read_delim("picrust2/picrust_placeseqs_closests_ref_sequences_tsv.tsv", # read the tsv file
                            delim = "\t", escape_double = FALSE, 
                            trim_ws = TRUE)

picrust_closestref %>%
    ggplot()+
    geom_point(aes(x=NSTI,y=`%cov`,color="darkorange"))+
    geom_point(aes(x=NSTI,y=`%id`,color="darkorchid4"))+
    theme_classic2()+
    ylab("Metric %") +
     scale_color_identity(name = "Blast metrics",
                          breaks = c("darkorange", "darkorchid4"),
                          labels = c("Coverage", "Identity"),guide = "legend")+ 
    scale_x_continuous(breaks = scales::breaks_width(2),expand = c(0, 0))+
    ggplot(picrust_closestref)+
    geom_point(aes(x=NSTI,y=score), color="aquamarine4")+
    theme_classic2()+
    ylab("Blast score")
```

Clusters with NSTI > 2 are considered poorly represented in the Picrust2 reference database.
Additionnaly, a coverage < 50% is pretty low 

```{r}
(picrust_closestref %>%
    ggplot(aes(x=score,y=`%id`))+
    geom_point()+
    picrust_closestref %>%
    ggplot(aes(x=`%id`,y=`%cov`))+
    geom_point()+
    picrust_closestref %>%
    ggplot(aes(x=score,y=`%cov`))+
    geom_point())/
    (picrust_closestref %>%
         filter(`%cov`>80&`%id`>80)%>%
    ggplot(aes(x=score,y=`%id`))+
    geom_point()+
    picrust_closestref %>%
         filter(`%cov`>80&`%id`>80)%>%
    ggplot(aes(x=`%id`,y=`%cov`))+
    geom_point()+
    picrust_closestref %>%
         filter(`%cov`>80&`%id`>80)%>%
    ggplot(aes(x=score,y=`%cov`))+
    geom_point())
```

```{r}
picrust_closestref %>%
         filter(`%cov`>80&`%id`>80)%>%
    nrow()/nrow(picrust_closestref) # 93% of OTUs

picrust_closestref %>%
         filter(`%cov`>80&`%id`>80)%>%
    summarise(sum_seq=sum(`Nb sequences`))/sum(picrust_closestref$`Nb sequences`) # Keep 98% of sequences
```

I don't have much confidence in these ASVs associated predictions cuz poorly mapped in Picrust2 reference db so filter them out.

```{r}
asvs_picrust <- picrust_closestref %>%
         filter(`%cov`>80&`%id`>80)%>%select(`#ASV`)
asvs_picrust <- asvs_picrust$`#ASV`
```


```{r}
meco_16s_picrust2 <- clone(meco_16s)

meco_16s_picrust2$otu_table %<>% filter(rownames(.)%in%asvs_picrust)
meco_16s_picrust2$tidy_dataset()
```

```{r}
ab_trans_meco_16s_picrust2 <- trans_abund$new(meco_16s_picrust2,taxrank = "Order",ntaxa = 8)
order_relab_16s_picrust2 <- ab_trans_meco_16s_picrust2$plot_bar(others_color = "grey70", xtext_keep = FALSE, legend_text_italic = FALSE)+
    geom_col(width = 1)+
    ggtitle("MAPP 16s")+
    theme(plot.title = element_text(face='bold'))
```

It changes a bit my 


## Functions and pathways

We investigate functions and pathways using two distinct databases (Kegg and MetaCyc).

```{r}
pathways_db <- c("Kegg","MetaCyc")
```


```{r}
for(i in pathways_db){
    
    # get file from good database
    shrtct <- ifelse(i=="Kegg","KO","EC")
    
    # load function copy number predictions
    picrust_fct_pred_copynb <- read_delim(paste0("picrust2/picrust_fct__",shrtct,"_copynumbers_predicted_tsv.tsv"),
                                              delim = "\t", escape_double = F, trim_ws = T)
    # load function unstrat dataset
    picrust_fct_unstrat <- read_delim(paste0("picrust2/picrust_fct_unstrat_",shrtct,"_tsv.tsv"),
                                              delim = "\t", escape_double = F, trim_ws = T)
    # load normalize marker abundance
    picrust_norm_marker <- read_delim(paste0("picrust2/picrust_fct_marker_norm_tsv.tsv"),
                                              delim = "\t", escape_double = F, trim_ws = T)
}


```


```{r}
kegg_pi <- read_delim("../../ressource/Picrust2/Galaxy103-[FROGSFUNC_2_functions__KO_copynumbers_predicted.tsv].tsv", # read the tsv file
                            delim = "\t", escape_double = FALSE, 
                            trim_ws = TRUE)

kegg_pi2 <- read_delim("../../ressource/Picrust2/Galaxy109-[FROGSFUNC_2_functions___frogsfunc_functions_unstrat_KO.tsv].tsv", # read the tsv file
                            delim = "\t", escape_double = FALSE, 
                            trim_ws = TRUE)

kegg_pi3 <- read_delim("../../ressource/Picrust2/Galaxy99-[FROGSFUNC_2_functions__frogsfunc_functions_marker_norm.tsv].tsv", # read the tsv file
                            delim = "\t", escape_double = FALSE, 
                            trim_ws = TRUE)
```

Rows of Kegg_pi2 correspond to non null columns of Kegg_pi 


```{r}
names(kegg_pi2)[grepl("L001",names(kegg_pi2))] <- gsub("[A-Z]|-|(?=_).*","",names(kegg_pi2)[grepl("L001",names(kegg_pi2))] ,perl = T)

kegg_pi2%>%
    select(c(1:4,which(names(.)%in%site_data$Sample)))%>%
    filter(grepl("glucosidase",classification))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c4))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values =RColorBrewer::brewer.pal(11,'Set3'))+
    ylab("Pathway %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
    kegg_pi2%>%
    select(c(1:4,which(names(.)%in%site_data$Sample)))%>%
    filter(grepl("aminopep",classification))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c4))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values =c("#4e8937",
                                "#733cc9",
                                "#6dc140",
                                "#cf4cc3",
                                "#63b889",
                                "#d24172",
                                "#4ca3ae",
                                "#d74c33",
                                "#656ad1",
                                "#cf9133",
                                "#6d2f7d",
                                "#a6a34d",
                                "#bb86cb",
                                "#4a5c2f",
                                "#6f97d0",
                                "#814025",
                                "#434a7b",
                                "#cf8c70",
                                "#78334b",
                                "#d0849f"))+
    ylab("Enzymes")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))
```

```{r}
(EC_pi2%>%
    select(c(1:4,which(names(.)%in%enzymes_data$Sample)))%>%
    # mutate(across(5:ncol(.),~(.x/observation_sum)*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    filter(grepl("leucyl aminopep",classification))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value))+
    geom_bar(stat = "identity",width=1,fill="#d74c33")+
    ylab("Picrust2 leucyl aminopep")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
    enzymes_data%>%
    ggplot(aes(x=Sample,y=LAP))+
    geom_bar(stat='identity',fill="#d74c33",width=1)+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0)))/
    (EC_pi2%>%
    select(c(1:4,which(names(.)%in%enzymes_data$Sample)))%>%
    # mutate(across(5:ncol(.),~(.x/observation_sum)*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    filter(grepl("beta-glucosidase",classification))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value))+
    geom_bar(stat = "identity",width=1,fill="#656ad1")+
    ylab("Picrust2 beta-glucosidase")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
    enzymes_data%>%
    ggplot(aes(x=Sample,y=BG))+
    geom_bar(stat='identity',fill="#656ad1",width=1)+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0)))

```


```{r}
EC_pi2%>%
    select(c(1:4,which(names(.)%in%mutate(meco_16s$sample_table,name=gsub("[A-Z]|-|(?=_).*","",sample_id ,perl = T))$Sample)))%>%
    # mutate(across(5:ncol(.),~(.x/observation_sum)*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    group_by(name)%>%
    summarise(val=sum(value))%>%
    left_join(mutate(meco_16s$sample_table,name=gsub("[A-Z]|-|(?=_).*","",sample_id ,perl = T)))%>%
    ggplot(aes(val,nb_reads))+
    geom_point()+
    xlab("Picrust2 copy estimation")
```


```{r}
path_EC_pi3 <- read_delim("../../ressource/Picrust2/Galaxy115-[FROGSFUNC_3_pathways__frogsfunc_pathways_unstrat.tsv].tsv", # read the tsv file
                            delim = "\t", escape_double = FALSE, 
                            trim_ws = TRUE)
```

```{r}
names(path_EC_pi3)[grepl("L001",names(path_EC_pi3))] <- gsub("[A-Z]|-|(?=_).*","",names(path_EC_pi3)[grepl("L001",names(path_EC_pi3))] ,perl = T)
```


```{r}
path_EC_pi3%>%
    select(c(1:4,which(names(.)%in%site_data$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c1))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values =RColorBrewer::brewer.pal(7,'Dark2'))+
    ylab("Pathway %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))
    
```

```{r}
path_EC_pi3%>%
    select(c(1:4,which(names(.)%in%site_data$Sample)))%>%
    filter(grepl("Degradation;",classification))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c2))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values =RColorBrewer::brewer.pal(11,'Set3'))+
    ylab("Pathway %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))
```
