---
title: "R Notebook"
output: html_document
---

# Intro

The aim of this paper is to investigate microbial communities from northern Peatlands by mean of 16s Metabarcoding.
This investigation is both Taxonomic and Functional.

For about 200 hundred samples from the northern hemisphere we have:

- *Communities info*:  
    * Metabarcoding (16s - bacteria/archae)  
    * Bacteria and photoautotrophs abundances (cytometry)  
    * Enzyme activity (C, N and P cycles; 4 enzymes)  
- *Site bioclimatic data*:  
    * Coordinates  
    * Google Earth Engine (GEE) data  
    

## Metabarcoding

Raw sequencing output (ILLUMINA) were uploaded to Galaxy Toulouse server to be processed using FROGS 4.1 bioinformatic pipeline available on the platform.

### FROGS pipeline

#### I - Preprocess

**Sequencer**: ILLUMINA  
**Input type**: .TAR archive  
**File**: .TAR archive with \*R1.fastq.gz and \*R2.fastq.gz for each sample  
**Merged reads**: NO  
*Read 1 &2 size*: 250  
**Mismatch rate**: 0.1  
**Merge software**: Vsearch  
**Keep unmerged**: NO  
**Amplicon size**: min 200 - max 490  
**Primers**: YES  
**5'**: Forward primer sequence (GTGYCAGCMGCCGCGGT)  
**3'**: Reversed & complemented (ACTYAAAKGAATTGRCGGGG) (online: https://reverse-complement.com/)  

#### II - Clustering Swarm

With fastidious method & d=1 : effectively ASVs

**Aggregation distance**: 1  
**Refine clustering**: YES


#### III - Remove chimera


#### IV - Cluster Filtering

**Minimum prevalence**: NA  
**Minimum proportion/count**: 2  -- Remove rare (proportion) or ASVs with less than X sequences (X=2 => remove singletons)  
**Search contaminant**: On server PhiX database

#### V - Taxonomic Affiliation

Chose ref db (SILVA or Âµgreen)
Ask for RDP classifier too!


**Blast metrics**:

- **Query Coverage**: Percent of the query sequence length that is included in alignments against the sequence match.

- **E-value**: Indicates the number of hits or alignments that are expected to be seen by random chance with the same score or better. 
The lower the E-value, the more significant the alignment (the closer to 0, the better).
E-value is the default metric used to sort the Descriptions table. 

- **Percent Identity**: Percent of nucleotides or amino acids that are identical between the aligned query and database sequences. 
A query sequence can share low percent identity with a sequence and still be a significant hit. 
It is essential to take the E-value into account and look for similarity between conserved regions (this will be more evident at the amino acid level).


#### VI - Postprocess

NOT filtering assignation within FROGS
Produce assignation stats
Convert .BIOM to .TSV and give a sequence file to keep sequence info!! (More universal format even tho .BIOM is loadable into R for further analysis)

### FROGS Picrust2

[Picrust 2](https://huttenhower.sph.harvard.edu/picrust/) or Phylogenetic Investigation of Communities by Reconstruction of Unobserved States is a software for predicting functional abundances based only on marker gene sequences.

It allows for functional characterization of your microbial communities from 16s metabarcoding data.
We ran Picrust 2 from within the Galaxy Toulouse platform with their pipeline names FROGSFUNC.
The process is divided in three steps that we'll describe briefly but first we aggregate ASVs based on their similarity ,shared taxonomy and sequence coverage.
This step is named FROGS affiliation postprocess on Galaxy Toulouse.

#### Affiliation Postprocess

**Input**: FASTA file containing our sequence and BIOM file with ASVs abundance and taxonomy.  
**Is this amplicon hyper variable length**: NO
**Minimum Identity**: ASVs will be aggregated if they share the same taxonomy AND with at least X% identity. We chose X = 99.  
**Minimum Coverage**: ASVs will be aggregated if they share the same taxonomy AND with at least X% alignment coverage. We chose X = 99.  

#### I - Place sequence and copy numbers

It is the first step of PICRUSt2.
It inserts your study sequences into a reference tree.
By default, this reference tree is based on 20,000 16S sequences from genomes in the Integrated Microbial Genomes database.
The script performs this step, which specifically:
    * Aligns your study sequences with a multiple-sequence alignment of reference 16S, ITS or 18S sequences with HMMER.
    * Finds the most likely placements of your study sequences in the reference tree with EPA_NG or SEPP.
    * Produces a treefile with the most likely placement for each sequence as the new tips with GAPPA.
    * Predicts marker copy number based solely on the sequences of marker genes with PICRUSt2. The available marker genes are 16S, ITS and 18S.


**Input**: FASTA sequence file and BIOM file with ASVs abundance and taxonomy
**Taxonomy marker**: 16s  
**Placement Tool**: epa-ng
**Minimum alignment length**: 0.8 - Proportion of total length of an iput sequence that must align with reference sequences.
All other will be out.

#### II - Calculate Function abundance

It is the second step of PICRUSt2. It ables to predicts :
    * Functional abundances based solely on the sequences of marker genes with PICRUSt2. The available marker genes are 16S, ITS and 18S.
    * Functions, weighted by the relative abundance of ASVs in the community. Inferring the metagenomes of the communities with PICRUSt2.

There are three steps performed at this stage:
    * It runs hidden-state prediction (hsp) to predict function abundances with castor-R of each ASVs placed in the PICRUSt2 reference phylogenetic tree (FROGSFUNC_1_placeseqs_copynumber outputs).
    * The read depth per ASV is divided by the predicted marker (16S/ITS/18S) copy numbers. This is performed to help control for variation in marker copy numbers across organisms, which can result in interpretation issues. For instance, imagine an organism with five identical copies of the 16S gene that is at the same absolute abundance as an organism with one 16S gene. The ASV corresponding to the first organism would erroneously be inferred to be at higher relative abundance simply because this organism had more copies of the 16S gene.
    * The ASV read depths per sample (after normalizing by marker (16S/ITS/18S) copy number) are multiplied by the predicted function copy numbers per ASV.

**Input**: BIOM and FASTA files + TREE file (.nwk) and the MARKER file (.tsv) containing the copy number of the marker produced by the previous step.  
**Taxonomic marker**: 16s  
**Target function database**: Select all  
**NTSI cut-off**: 2 - Any sequence with NTSI > 2 is discarded. Nearest Sequenced Taxon Index (NSTI) is the phylogenetic distance between the ASV and the nearest sequenced reference genome. 
**Identity alignment cut_off**: 0 - Any sequence below this identity threshold agaisnt reference will be discarded.
**Coverage cut-off**: 0 - Same with coverage.
**HSP method**: mp - Hidden state prediction method used. We stick with the maximum parsimony default.

#### III - Calculate Pathways abundance

It is the last step of PICRUSt2. This script infers MetaCyc/KEGG pathway abundances based on EC or KO number abundances.
    * Regroups EC or KO numbers to MetaCyc or KEGG reactions, depending of the unstrat abundances input file.
    * Infers which MetaCyc or KEGG pathways are present based on these reactions with MinPath.
    * Calculates and returns the abundance of pathways identified as present.

Run twice, once for MetaCyc and once for KEGG.
 
 **Input**: TSV file with function abundance prediction from step II (EC - metacyc and KO - kegg).
 **Taxonomic marker**: 16s
 **Pathway reference**: MetaCyc and then Kegg according to the input file (EC - metacyc and KO - kegg).


# Load Packages & custom function

```{r}
EcophyCofog::Library(c("EcophyCofog","metabaR","readr","dplyr","magrittr","ggplot2","reshape2","patchwork","ggpubr","tidyr","forcats","phyloseq","cowplot","readxl","microeco","file2meco","RColorBrewer","vegan","sf","rnaturalearth","rnaturalearthdata","dendextend","GGally","ggvegan"))
```

```{r}
createAngleHJustCols <- function(labeldf) {        
    nn <- length(labeldf$y)
    halfn <- floor(nn/2)
    firsthalf <- rev(90 + seq(0,360, length.out = nn))
    secondhalf <- rev(-90 + seq(0,360, length.out = nn))
    angle <- numeric(nn)
    angle[1:halfn] <- firsthalf[1:halfn]
    angle[(halfn+1):nn] <- secondhalf[(halfn+1):nn]

    hjust <- numeric(nn)
    hjust[1:halfn] <- 0
    hjust[(halfn+1):nn] <- 1

    return(list(angle = angle, hjust = hjust))
}


fviz_nbclust_fixed <- function (x, FUNcluster = NULL, method = c("silhouette", "wss", 
    "gap_stat"), diss = NULL, k.max = 10, nboot = 100, verbose = interactive(), 
    barfill = "steelblue", barcolor = "steelblue", linecolor = "steelblue", 
    print.summary = TRUE, ...) 
{
    set.seed(123)
    if (k.max < 2) 
        stop("k.max must bet > = 2")
    method = match.arg(method)
    if (!inherits(x, c("data.frame", "matrix")) & !("Best.nc" %in% 
        names(x))) 
        stop("x should be an object of class matrix/data.frame or ", 
            "an object created by the function NbClust() [NbClust package].")
    if (inherits(x, "list") & "Best.nc" %in% names(x)) {
        best_nc <- x$Best.nc
        if (any(class(nb$Best.nc) == "numeric")) 
            print(best_nc)
        else if (any(class(nb$Best.nc) == "matrix")) 
            .viz_NbClust(x, print.summary, barfill, barcolor)
    }
    else if (is.null(FUNcluster)) 
        stop("The argument FUNcluster is required. ", "Possible values are kmeans, pam, hcut, clara, ...")
    else if (!is.function(FUNcluster)) {
        stop("The argument FUNcluster should be a function. ", 
            "Check if you're not overriding the specified function name somewhere.")
    }
    else if (method %in% c("silhouette", "wss")) {
        if (is.data.frame(x)) 
            x <- as.matrix(x)
        if (is.null(diss)) 
            diss <- stats::dist(x)
        v <- rep(0, k.max)
        if (method == "silhouette") {
            for (i in 2:k.max) {
                clust <- FUNcluster(x, i, ...)
                v[i] <- .get_ave_sil_width(diss, clust$cluster)
            }
        }
        else if (method == "wss") {
            for (i in 1:k.max) {
                clust <- FUNcluster(x, i, ...)
                v[i] <- .get_withinSS(diss, clust$cluster)
            }
        }
        df <- data.frame(clusters = as.factor(1:k.max), y = v, 
            stringsAsFactors = TRUE)
        ylab <- "Total Within Sum of Square"
        if (method == "silhouette") 
            ylab <- "Average silhouette width"
        p <- ggpubr::ggline(df, x = "clusters", y = "y", group = 1, 
            color = linecolor, ylab = ylab, xlab = "Number of clusters k", 
            main = "Optimal number of clusters")
        if (method == "silhouette") 
            p <- p + geom_vline(xintercept = which.max(v), linetype = 2, 
                color = linecolor)
        return(p)
    }
    else if (method == "gap_stat") {
        extra_args <- list(...)
        gap_stat <- cluster::clusGap(x, FUNcluster, K.max = k.max, 
            B = nboot, verbose = verbose, ...)
        if (!is.null(extra_args$maxSE)) 
            maxSE <- extra_args$maxSE
        else maxSE <- list(method = "firstSEmax", SE.factor = 1)
        p <- fviz_gap_stat(gap_stat, linecolor = linecolor, 
            maxSE = maxSE)
        return(p)
    }
}



.viz_NbClust <- function(x, print.summary = TRUE,
                         barfill = "steelblue", barcolor = "steelblue")
  {
     best_nc <- x$Best.nc
    if(any(class(best_nc) == "numeric")) print(best_nc)
     else if(any(class(best_nc) == "matrix")){
    best_nc <- as.data.frame(t(best_nc), stringsAsFactors = TRUE)
    best_nc$Number_clusters <- as.factor(best_nc$Number_clusters)
    
    # Summary
    if(print.summary){
      ss <- summary(best_nc$Number_clusters)
      cat ("Among all indices: \n===================\n")
      for(i in 1 :length(ss)){
        cat("*", ss[i], "proposed ", names(ss)[i], "as the best number of clusters\n" )
      }
      cat("\nConclusion\n=========================\n")
      cat("* According to the majority rule, the best number of clusters is ",
          names(which.max(ss)),  ".\n\n")
    }
    df <- data.frame(Number_clusters = names(ss), freq = ss, stringsAsFactors = TRUE )
    p <- ggpubr::ggbarplot(df,  x = "Number_clusters", y = "freq", fill = barfill, color = barcolor)+
      labs(x = "Number of clusters k", y = "Frequency among all indices",
           title = paste0("Optimal number of clusters - k = ", names(which.max(ss)) ))
    
    return(p)
  }
}



cor_fun <- function(data, mapping, method="pearson", use="pairwise", sz=5, ...){
    
    # grab data
    x <- eval_data_col(data, mapping$x)
    y <- eval_data_col(data, mapping$y)
    
    # calculate correlation: for significance stars
    corr <- cor.test(x, y, method=method)
    est <- corr$estimate
    
    # get significance stars

        stars <- c("***", "**", "*", "")[findInterval(corr$p.value, c(0, 0.001, 0.01, 0.05, 1))]
        lbl <- stars

    # calculate correlation: for colored tiles
    corr <- cor(x, y, method=method, use=use)
    
    # calculate color based on correlation value
    # corr = -1 => blue, 
    # corr =  0 => white, 
    # corr = +1 => red, 
    colFn <- colorRampPalette(c("blue", "white", "red"), interpolate ='spline')
    fill <- colFn(100)[findInterval(corr, seq(-1, 1, length=100))]
    fill[which(stars=="")] <- "grey"
    ggplot(data = data, mapping = mapping, ...) + 
        theme_void() +
        annotate("text",
                 x=mean(x, na.rm=TRUE),
                 y=mean(y, na.rm=TRUE),
                 label=lbl,
                 ...) +
        theme(panel.background = element_rect(fill=fill,  # to fill background of panel with color
                                              colour=NA), # to remove border of panel
              panel.grid.major = element_blank())
}
```

# MetabaR Processing

## prepare metabarlist

```{r}
data_temp <- read_delim(file = "frogs_input/Galaxy169-[FROGS_BIOM_to_TSV__abundance.tsv].tsv", # read the tsv file
                        delim = "\t", escape_double = FALSE, 
                        trim_ws = TRUE)

reads_frame <- t(as.matrix(data_temp[,colnames(data_temp)[grepl("L001",colnames(data_temp))]])) # extract site per asvs reads count matrix
colnames(reads_frame) <- data_temp$observation_name #set colnames as cluster ID (asv 'names')


motus_frame <- data_temp[,colnames(data_temp)[!grepl("L001",colnames(data_temp))]] # extract asv information (taxonomy, counts etc)


motus_frame %<>% dplyr::rename(sequence=seed_sequence) # rename seed_sequence columns to sequence
motus_frame %<>% mutate(rdp_tax_and_bootstrap=gsub("\\(|\\)|;$","",rdp_tax_and_bootstrap,perl=T))

motus_frame %<>% mutate(blast_taxonomy=ifelse(blast_taxonomy=="no data","k__;p__;c__;o__;f__;g__;s__",blast_taxonomy))
motus_frame %<>% separate(rdp_tax_and_bootstrap,
                          into=c("Kingdom","boots_Kingdom",
                                 "Phylum","boots_Phylum",
                                 "Class","boots_Class",
                                 "Order","boots_Order",
                                 "Family","boots_Family",
                                 "Genus","boots_Genus",
                                 "Species","boots_Species"),
                          sep=";",
                          remove = F)%>%
    separate(blast_taxonomy,
             into=c("Kingdom_blast",
                    "Phylum_blast",
                    "Class_blast",
                    "Order_blast",
                    "Family_blast",
                    "Genus_blast",
                    "Species_blast"),
             sep=";",
             remove = F)%>%
    mutate(across(.cols=which(grepl("boots_",colnames(.))),.fns=as.numeric))

rownames(motus_frame) <- motus_frame$observation_name # use asv names as rownames of the df
pcr_frame <- data.frame(sample_id=colnames(data_temp)[grep("L001",colnames(data_temp))], type="sample",control_type=NA) # set pcr frame as required by metabaR
rownames(pcr_frame) <- pcr_frame$sample_id # set rownames as site/sample id

sample_frame <- data.frame(sample_id=colnames(data_temp)[grep("L001",colnames(data_temp))]) # create sample frame
rownames(sample_frame) <- sample_frame$sample_id # set rownames as site/sample id

metab_list <- metabarlist_generator(reads = reads_frame, # create metabarlist
                                    motus = motus_frame,
                                    pcrs = pcr_frame,
                                    samples = sample_frame)

rm(data_temp,motus_frame,pcr_frame,reads_frame,sample_frame) # remove temporary files
```



## Start MetabaR processing


### Evaluate PCRs on sequencing depths

Plot number of ASVs and reads per PCRs as well as their correlation.
Correlation between reads and richness are problematic as they might indicate insufficient sequencing depth.
```{r}
data_temp <- metab_list# get one metabarlist

# Compute the number of reads per pcr
data_temp$pcrs$nb_reads <- rowSums(data_temp$reads)

# Compute the number of asvs per pcr
data_temp$pcrs$nb_asvs <- rowSums(data_temp$reads>0)
metab_list <- data_temp
check1 <- reshape2::melt(data_temp$pcrs[,c("control_type", "nb_reads", "nb_asvs")])

p1 <- ggplot(data <- check1, aes(x=control_type, y=value, color=control_type)) + 
    geom_boxplot() + 
    theme_bw() + 
    geom_jitter(alpha=0.2) + 
    scale_color_manual(values = c("brown", "red", "cyan4","pink"), na.value = "darkgrey") +
    facet_wrap(~variable, scales = "free_y") +
    ggtitle("16s")+ 
    theme(axis.text.x = element_text(angle=45, h=1),
          plot.title = element_text(face="bold"))

p2 <-  ggplot(data_temp$pcrs, aes(x=nb_reads, y=nb_asvs, color = control_type)) + 
    geom_point() + 
    theme_bw() + 
    scale_y_log10() + 
    scale_x_log10() + 
    scale_color_manual(values = c("brown", "red", "cyan4","pink"), na.value = "darkgrey")

diagplot <- p1 + p2 + plot_layout(guides="collect") # sample 124 low reads?
rm(p1,p2,data_temp,check1,data)
diagplot
```

```{r}
ggsave("Figures/reads_and_asvs_raw.png",diagplot,width = 14)
```


All samples but one have more than 10000 reads. 
Remove this outlier

Flag pcrs according to seq depths
```{r}
data_temp <- metab_list

# Tag as ok pcrs with more than 10 000 reads
data_temp$pcrs$seqdepth_ok <- ifelse(data_temp$pcrs$nb_reads < 10e3, F, T)

# Overwrite
metab_list <- data_temp 

# proportion of innaceptable pcrs seq depth, control excluded!!
seq_depth_tab <- table(data_temp$pcrs$seqdepth_ok[data_temp$pcrs$type=="sample"]) /
    nrow(data_temp$pcrs[data_temp$pcrs$type=="sample",])
print(seq_depth_tab)
rm(data_temp)
```


### Remove non target ASVs

Tag non target ASVs
```{r}
non_target_prop <- NULL
non_targetconta_prop <- NULL

data_temp <- metab_list

#Flag ASVs corresponding to target (TRUE) vs. non-target (FALSE) taxa 
data_temp$motus$target_taxon <- grepl("Bacteria|Archaea", data_temp$motus$Kingdom)&!grepl("Chloroplast",data_temp$motus$Order)&!grepl("Mitochondria",data_temp$motus$Family)  # MITO and rickettsiales
non_targ_asvs <- data_temp$motus$observation_name[which(data_temp$motus$target_taxon==F)]
# Proportion of each of these over total number of ASVs
non_target_prop <- table(data_temp$motus$target_taxon) / nrow(data_temp$motus)
print(non_target_prop)
# Intersection with extraction contaminant flags (not contaminant = T)
non_target_prop <- table(data_temp$motus$target_taxon)
print(non_target_prop)

# Overwrite
metab_list <- data_temp
```


### Filter and format according to Assignment quality


Tag as poorly assigned Sequences with no assignment at the kingdom lvl (ie with RDP bootstrap values <70: boots_thshld choosen earlier) 
```{r}
boots_thshld <- 0.7 # define a RDP classifier bootstrap value under which we consider affiliation uncertain

# test <- NULL
data_temp <- metab_list
melty <- melt(data_temp$motus, id = c(colnames(data_temp$motus)[which(grepl("boots_",colnames(data_temp$motus))==F)] ))
melty$variable <- gsub("boots_","",melty$variable)
melty$variable <- as.factor(melty$variable)

melty$variable <- fct_relevel(melty$variable,"Kingdom","Phylum","Class","Order","Family","Genus","Species")


plot_tax_boots <-  ggplot(melty,aes(x=variable,y=as.numeric(value))) +
    geom_boxplot(aes(fill=variable))+
    theme_bw()+
    ggtitle("16s")+
    labs(y="conf %")+
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.title.x = element_blank())+
    theme(legend.position = "n",
          plot.title = element_text(face = "bold"))

# Create a "confident taxonomy" (boots >= boots_thshld)
colnames_tmp <- c(gsub("boots_","",grep("boots_",colnames(data_temp$motus), value = T)))

for(j in 1:length(colnames_tmp)){
    
    col <- match(colnames_tmp[j],colnames(data_temp$motus))
    a <- data_temp$motus[,c(col,col+1)]
    tax_lvl <- paste0(tolower(substr(names(a)[2],7,7)),"__")
    conf <- a %>% mutate(col_temp=ifelse(.[[2]]<boots_thshld,gsub(".*",tax_lvl,.[[1]],perl=T),.[[1]]))
    data_temp$motus <- cbind(data_temp$motus,conf[,3])
    colnames(data_temp$motus)[ncol(data_temp$motus)]<- paste0(colnames_tmp[j],"_conf_rdp")
}

# Tag sequences not assigned below Kingdom
data_temp$motus %<>% mutate(good_assign = ifelse(Phylum_conf_rdp=="p__",F,T))

# Overwrite
metab_list <- data_temp
rm(data_temp,melty)

plot_tax_boots
```

### Display noise in ASVs

summarize noise in ASVs
```{r}
# color_scale (create a common scale across datasets)
a <- unlist(combn(c("untargeted_taxon","poorly_assigned"),1,simplify = F))
b <- c("poorly_assigned|untargeted_taxon")
myColors <- RColorBrewer::brewer.pal(8,"Set1")
names(myColors) <- levels(as.factor(c(a,b,"not_artefactual")))
colScale <- scale_fill_manual(name = "grp",values = myColors, drop=F)

data_temp <- metab_list

# Create a table of ASVs quality criteria 
# noise is identified as FALSE in data_temp, the "!" transforms it to TRUE 
asvs.qual <- !data_temp$motus[,c("target_taxon","good_assign")] #not_degraded 
colnames(asvs.qual) <- c("untargeted_taxon","poorly_assigned") #degraded_seq  

asv_noise_tab <- NULL
# Proportion of asvs potentially artifactual (TRUE) based on the criteria used
asv_noise_tab$motus <- prop.table(table(apply(asvs.qual, 1, sum) > 0))

# Corresponding proportion of artifactual reads (TRUE)
asv_noise_tab$reads <- prop.table(xtabs(data_temp$motus$observation_sum~apply(asvs.qual, 1, sum) > 0))

# Proportion of asvs and reads potentially artifactual for each criterion
apply(asvs.qual, 2, sum) / nrow(asvs.qual)
apply(asvs.qual, 2, function(x) sum(data_temp$motus$observation_sum[x])/sum(data_temp$motus$observation_sum))

tmp.asvs <- 
    apply(sapply(1:ncol(asvs.qual), function(x) {
        ifelse(asvs.qual[,x]==T, colnames(asvs.qual)[x], NA)}), 1, function(x) {
            paste(sort(unique(x)), collapse = "|")
        })
tmp.asvs <- as.data.frame(gsub("^$", "not_artefactual", tmp.asvs))
colnames(tmp.asvs) <-  "artefact_type"
tmp.asvs %<>% mutate(artefact_type=as.factor(artefact_type)  )

plot_noise_asv <- ggplot(tmp.asvs, aes(x=1, fill=artefact_type)) +
    geom_bar() +  xlim(0, 2) +
    labs(fill="Artifact type") + 
    coord_polar(theta="y") + theme_void() + 
    scale_fill_manual(name = "grp",values = myColors, drop=F) + 
    theme(legend.direction = "vertical") + 
    ggtitle("16s - ASVS noise")+
    theme(plot.title = element_text(face="bold"))

rm(tmp.asvs,asvs.qual,data_temp,myColors,colScale,a,b)

plot_noise_asv
```

```{r}
ggsave("Figures/noise_in_asvs.png",plot_noise_asv)
```


### Filter 


```{r}
# Use tag-jump corrected metabarlist with the threshold identified above
tmp <- metab_list

# Subset onASVs: we keep asvs that are defined as TRUE following the 
# criteria below (sum of x TRUE is equal to x with the rowSums function)
row.names(tmp$motus) <- colnames(tmp$reads)

tmp <- subset_metabarlist(tmp, "motus", 
                          indices = rowSums(tmp$motus[,c("good_assign", "target_taxon")]) == 2)# remove untargeted taxa and poorly assigned (no assignation @phylum)

# Subset on pcrs and keep only samples 
data_temp <- subset_metabarlist(tmp, "pcrs", 
                                indices = tmp$pcrs[,c("seqdepth_ok")] == TRUE & #, "replicating_pcr" 
                                    tmp$pcrs$type == "sample")

#update counts and reads.    
data_temp$motus$counts = colSums(data_temp$reads)
data_temp$pcrs$reads_post = rowSums(data_temp$reads)
data_temp$pcrs$asvs_post = rowSums(ifelse(data_temp$reads>0, T, F))

check <- melt(data_temp$pcrs[,c("nb_reads", "reads_post", 
                                "nb_asvs", "asvs_post")])
check$type <- ifelse(grepl("asvs", check$variable), "richness", "abundance")

summary_list <- summary_metabarlist(data_temp)

if(sum(colSums(data_temp$reads)==0)>0){print(paste0(folder_names[[i]]," empty asvs present"))}
if(sum(colSums(data_temp$reads)==0)>0){print(paste0(folder_names[[i]]," empty pcrs present"))}

sumpipeline_plots <- ggplot(data = check, aes(x = variable, y = value)) +
    geom_boxplot( color = "darkgrey") +
    geom_jitter(alpha=0.1, color = "darkgrey") +
    theme_bw() +
    facet_wrap(~type, scales = "free", ncol = 5) +
    theme(axis.text.x = element_text(angle=45, h=1),
          axis.title = element_blank()) +
    ggtitle("Effect of cleaning")

cleaned_metablist <- data_temp
rm(tmp,data_temp,check)
sumpipeline_plots
```

```{r}
ggsave("Figures/effect_of_cleaning_asvs_reads.png",sumpipeline_plots,width=14)
```

### Check effect of filter on reads and richness


```{r}
# Using the nb_reads and nb_asvs defined previously in the data_temp$pcrs table
data_temp <- cleaned_metablist

check1 <- reshape2::melt(data_temp$pcrs[,c("control_type", "reads_post", "asvs_post")])

p1 <- ggplot(data <- check1, aes(x=control_type, y=value, color=control_type)) + 
    geom_boxplot() + 
    theme_bw() + 
    geom_jitter(alpha=0.2) + 
    scale_color_manual(values = c("brown", "red", "cyan4","pink"), na.value = "darkgrey") +
    facet_wrap(~variable, scales = "free_y") +
    ggtitle("")+ 
    theme(axis.text.x = element_text(angle=45, h=1),
          plot.title = element_text(face="bold"))

p2 <-  ggplot(data_temp$pcrs, aes(x=reads_post, y=asvs_post, color = control_type)) + 
    geom_point() + 
    theme_bw() + 
    scale_y_log10() + 
    scale_x_log10() + 
    scale_color_manual(values = c("brown", "red", "cyan4","pink"), na.value = "darkgrey")+
    geom_smooth(method="lm",color="darkorange")

diag_plots_post <-  p1 + p2 + plot_layout(guides="collect")
rm(p1,p2,data_temp,check1,data)

diag_plots_post
```


```{r}
ggsave("Figures/cor_asvs_reads_postcleaning.png",diag_plots_post,width=14)
```


OK we have strong correlation between richness and reads within samples (deseq2 or rarefaction?)
Will depend on questions (test link between bioclimatic/geographical variables and the number of reads)
Differential abundance implementation in this models.

## Data wrangling

### Transform to physeq lists


```{r}
data_temp_cleaned <- cleaned_metablist
data_temp <- metab_list

rownames(data_temp$motus) <- colnames(data_temp$reads)
# Get row data only for samples
tmp <- subset_metabarlist(data_temp, table = "pcrs",
                          indices = data_temp$pcrs$type == "sample")

tmpcl <- subset_metabarlist(data_temp_cleaned, table = "pcrs",
                            indices = data_temp_cleaned$pcrs$type == "sample")

# Format for phyloseq

otumat <- as.matrix(tmp$reads)
taxmat <- as.matrix(tmp$motus)
sammat <- left_join(tmp$samples,tmp$pcrs)
OTU <- otu_table(otumat, taxa_are_rows = F)
TAX <- tax_table(taxmat)
SAM <- sample_data(sammat)
rownames(SAM) <- SAM$sample_id
physeq <- phyloseq(OTU,TAX,SAM)
physeq

otumatcl <- as.matrix(tmpcl$reads)
taxmatcl <- as.matrix(tmpcl$motus)
sammatcl <- left_join(tmpcl$samples,tmpcl$pcrs)
OTUcl <- otu_table(otumatcl, taxa_are_rows = F)
TAXcl <- tax_table(taxmatcl)
SAMcl <- sample_data(sammatcl)
rownames(SAMcl) <- SAMcl$sample_id
physeqcl <- phyloseq(OTUcl,TAXcl,SAMcl)
```


```{r}
# toremove <- grep("^metab|^cleaned|^physeq|non_targ_asvs|arrowMul", ls(), invert=T, value=T)
# rm(list = c(toremove, "toremove"))
# gc()
```


remove unwanted samples (Vincent treatments and chile sample)


```{r}
data_temp <- physeqcl
data_temp <- subset_samples(data_temp, !grepl("23[1-7]|239|240|241|042-047|047-042|246b",data_temp@sam_data$sample_id))
data_temp@sam_data$reads_post <- rowSums(data_temp@otu_table)

data_temp@sam_data$reads_post = rowSums(data_temp@otu_table) # reads per samples
data_temp@sam_data$asvs_post = rowSums(ifelse(data_temp@otu_table>0, T, F)) # motus per samples
physeqcl <- data_temp
```


### Transform to microeco

Load site metadata
```{r}
site_coordinates <- read.csv("../../ressource/Site_edaphic_data/site_coord_MAPP.csv")  # get site coordinates
site_coordinates %<>% 
    mutate(Sample=ifelse(nchar(Sample)<3,paste0("0",Sample),Sample))%>% # reformat 1 into 001 and 64 to 064 etc...
    mutate(Sample=ifelse(nchar(Sample)<3,paste0("0",Sample),Sample))
site_coordinates %<>% 
    mutate(Sample=ifelse(grepl("^8",Sample),paste0("0",Sample),Sample)) # fix 87a and 87b to 087a and 087b
```


```{r}
meco_16s <- phyloseq2meco(physeqcl)

meco_16s$sample_table %<>% mutate(Sample = gsub("[A-Z]|-|(?=_).*","",sample_id,perl = T))

# check for diff between sites coordinates obtained form vicent and samples remaining in the final analysis
setdiff(site_coordinates$Sample,meco_16s$sample_table$Sample) # we willingly removed 231-7|239|240|241 / 042 and 047 are mixed up what about : 115/124/199/200/201/205/206/208/209/227/252/253/258

# add coordinates to the three datasets
meco_16s$sample_table %<>% left_join(site_coordinates)

# reset rownames
rownames(meco_16s$sample_table) <- meco_16s$sample_table$sample_id

meco_16s$tidy_dataset() #tidy datasets

# Compute the number of reads per pcr
meco_16s$sample_table$nb_reads <- colSums(meco_16s$otu_table)

# Compute the number of asvs per pcr
meco_16s$sample_table$nb_reads <- colSums(meco_16s$otu_table>0)

gc()
```

# Explore 16s communities

Samples 256 and 87 are duplicated (a and b). 
256b and 87a are retained because of their higher sequencing depth.
```{r}
colSums(meco_16s$otu_table[,grepl("256",colnames(meco_16s$otu_table))]);colSums(meco_16s$otu_table[,grepl("87",colnames(meco_16s$otu_table))])
```

Moreover we lack geographical info about samples 116, 119 and 166 and samples 042 and 047 have been mixed and can't be exploited.

```{r}
meco_16s$otu_table <- meco_16s$otu_table[,!grepl("256a|87b|116|119|166",colnames(meco_16s$otu_table))] # remove these samples
meco_16s$filter_pollution(taxa = c("mitochondria", "chloroplast")) # remove chloroplast and mitochondria if some passed the previous filter
meco_16s$tidy_dataset() # tidy the microeco object
```

## Investigate singletons

Visualize number of singletons
```{r}
data_temp <- meco_16s # get data

n_single_asv <- sum(rowSums(data_temp$otu_table)==1) # number of single asvs (those that are present exactly once across all sites)
n_non_single_asv <- sum(rowSums(data_temp$otu_table)>1) # number of non single asvs
n_single_reads <- sum(data_temp$otu_table[which(rowSums(data_temp$otu_table)==1),]) # number of reads from the data set that correspond to asv singletons (sum of reads on singletons subset)
n_non_single_reads <- sum(data_temp$otu_table[which(rowSums(data_temp$otu_table)>1),]) # same but for non singletons

df_tmp <- data.frame(asv_val=c(n_single_asv,n_non_single_asv),
                     read_val=c(n_single_reads,n_non_single_reads),
                     type=c("n_single","n_non_single"))
df_tmp %<>% mutate(pct_asvs=asv_val/sum(asv_val), # convert to %
                   pct_reads=read_val/sum(data_temp$otu_table))%>%
    reshape2::melt()%>%
    filter(grepl("pct",variable))

singletons <- ggplot(df_tmp,aes(x = variable,y = value, fill=type))+
    geom_bar(position="stack",stat = "identity")+
    theme_classic2()+
    scale_fill_manual(values = c("lightseagreen","mediumvioletred"))+
    theme(plot.title = element_text(face='bold'))+
    ylab("% of singletons")+
    xlab("ASVs vs Reads")+
    scale_y_continuous(expand = c(0,0))


alpha <- colSums(data_temp$otu_table!=0) # sample alpha diversity as the number of non null rows (asvs)
single_asv <-  colSums(filter(data_temp$otu_table,rowSums(data_temp$otu_table)==1)==1) # number of singletons asvs within sample as the number of asvs restricted to this sample (subset rowsums) and with one read (==1)
# non_single_asv2 <- colSums(data_temp$otu_table>1)
non_single_asv <- alpha-single_asv # ASVs that might be singletons within the sample but not the dataset 
df_tmp <- data.frame(value = c(rbind(single_asv,non_single_asv)),
                     type=c("n_single","n_non_single"),
                     sample=rep(1:ncol(data_temp$otu_table),each=2))%>%
    group_by(sample)%>%
    mutate(val_pct= value/sum(value))

alpha_plot <- ggplot(df_tmp,aes(x=sample,y=value,fill=type))+
    geom_bar(stat="identity",position = "stack",width = 1)+
    theme_classic2()+
    ggtitle("16s singletons repartition")+
    theme(plot.title = element_text(face='bold'))+
    scale_y_continuous(expand = c(0,0))+
    scale_x_continuous(expand = c(0,0))+
    xlab("samples")+
    ylab("Richness")+
    scale_fill_manual(values=c("lightseagreen","mediumvioletred"))

alpha_plot_pct <- ggplot(df_tmp,aes(x=sample,y=val_pct,fill=type))+
    geom_bar(stat="identity",position = "stack",width = 1)+
    theme_classic2()+
    theme(plot.title = element_text(face='bold'))+
    scale_y_continuous(expand = c(0,0))+
    scale_x_continuous(expand = c(0,0))+
    xlab("samples")+
    ylab("Richness")+
    scale_fill_manual(values=c("lightseagreen","mediumvioletred"))


meco_16s_singl <- clone(meco_16s)
meco_16s_singl$otu_table %<>% filter(rowSums(.)==1)
meco_16s_singl$tidy_dataset()
meco_16s_singl$sample_table %<>% mutate(dumb_group = "sample")
meco_16s_singl$cal_abund()
meco_16s_singl <- trans_abund$new(meco_16s_singl,taxrank = "Order_conf_rdp",ntaxa = 8,groupmean = "dumb_group")
    
plot_data <- meco_16s_singl$data_abund
use_taxanames <- c(meco_16s_singl$data_taxanames,"unidentified")
plot_data$Taxonomy[!plot_data$Taxonomy %in% use_taxanames] <- "Others"
plot_data %<>% 
    dplyr::group_by(!!!syms(c("Taxonomy","Sample"))) %>% 
    dplyr::summarise(Abundance = sum(Abundance)) %>% 
    as.data.frame(stringsAsFactors = FALSE)
plot_data$Taxonomy %<>% factor(., levels = c(use_taxanames[-9], "Others", "unidentified"))
plot_data$label <- paste0(round(plot_data$Abundance, 1),"%")
donut_comp <- ggdonutchart(plot_data,"Abundance",
             fill="Taxonomy",
             label = "label",
             color = "white",
             palette = c(colorRampPalette(brewer.pal(8, "Dark2"))(9),"lightgrey"))+
    theme(legend.position = "right")


singleton_plots <- alpha_plot + theme(legend.position = "n") + alpha_plot_pct + singletons + theme(legend.position = "n") + donut_comp  + plot_layout(widths = c(1, 1))

ggsave("Figures/diagnose_singletons.png",singleton_plots,width = 14,height = 14)
```


We have only singletons in 16s but ~80% of our ASVs but ~4% of our reads.
Can this be interpretated as high endemism in peatlands bacteria??
And loads of rare strains? 
We are working at low resolution with SWARM clustering, increasing singletons.
We use the 'fastidious' option that should prevent to many singletons if they are close https://peerj.com/articles/1420/


Can't remove that many ASVs,huge effect on alpha div no?
Tho quite well distributed across samples.
Should reduce divergence between my samples.

Shall we keep singletons for alpha div and stuff but aggregate at some phylogenetical levels for modeling approaches? 
Hypothesis that phylogeny retains ecological features but is it true for bacteria? 



## Alpha-div

re-compute for each samples the sequencing depth and richness
```{r}
meco_16s$sample_table %<>%
    select(sample_id,nb_reads,nb_asvs,Sample,Y,X) %>%
    mutate(nb_reads=colSums(meco_16s$otu_table),
           nb_asvs=colSums(meco_16s$otu_table>0))
```


same without singletons
```{r}
meco_16s_ns <- clone(meco_16s)
meco_16s_ns$otu_table %<>% filter(rowSums(meco_16s_ns$otu_table>0)>1)
meco_16s_ns$tidy_dataset()
meco_16s_ns$sample_table %<>%
    mutate(nb_reads=colSums(meco_16s_ns$otu_table),
           nb_asvs=colSums(meco_16s_ns$otu_table>0))
```


```{r}
plot_alpha_divz_sgnsg <- NULL
for(i in c("meco_16s","meco_16s_ns")){
    
    tmp <- get(i)
    plot_title <- ifelse(match(i,c("meco_16s","meco_16s_ns"))==1,"With singletons","Without singletons")
    
    distrib_reads_and_asvs <- tmp$sample_table %>%
        ggplot(aes(x=nb_reads))+
        geom_histogram(bins=30,color='darkorange',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()+
        tmp$sample_table %>%
        ggplot(aes(x=nb_asvs))+
        geom_histogram(bins=30,color='deeppink4',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()
    
    cor_reads_asvs <- tmp$sample_table %>%
        ggplot(aes(x=nb_reads, y=nb_asvs))+
        geom_point(color='darkgrey')+
        scale_y_continuous(expand = expansion(mult = c(0,0.05), add= c(0,0)))+
        scale_x_continuous(expand = expansion(mult = c(0,0.05), add= c(0,0)) )+
        theme_classic2()+
        geom_smooth(method = 'lm',lty=2,color='darkorange')+
        ggpmisc::stat_poly_eq(ggpmisc::use_label(c("eq", "R2", "p")))
    
    tab16s <- t(otu_table(file2meco::meco2phyloseq(tmp))) # get the community
    class(tab16s) <- "matrix" # change class
    curve16s <- rarecurve(tab16s,step=100) # use vegan rarefaction curves
    names(curve16s) <- rownames(tab16s) # name curves after sample IDs
    
    # Coerce data into "long" form.
    protox <- mapply(FUN = function(x, y) {
        mydf <- as.data.frame(x)
        colnames(mydf) <- "value"
        mydf$samples <- y
        mydf$subsample <- attr(x, "Subsample")
        mydf
    }, x = curve16s, y = as.list(names(curve16s)), SIMPLIFY = FALSE)
    
    xy <- do.call(rbind, protox)
    rownames(xy) <- NULL  # pretty
    
    # Plot.
    rare16s <- ggplot(xy, aes(x = subsample, y = value, group = samples )) +
        theme_classic2() +
        scale_color_discrete(guide = "none") +  # turn legend on or off
        geom_line(color = "aquamarine4")+
        xlab("nb reads")+
        ylab("ASVs count")
    
    plot_alpha_divz_sgnsg[[i]] <- distrib_reads_and_asvs/(cor_reads_asvs+rare16s)+ plot_annotation(title = plot_title) & theme(plot.title = element_text(face='bold'))
    
}
plot_alpha_divz_sgnsg[[1]]|(plot_alpha_divz_sgnsg[[2]])
```

```{r}
ggsave("Figures/alpha_div_with_sing.png",plot_alpha_divz_sgnsg[[1]],width = 14,height = 14)
ggsave("Figures/alpha_div_no_sing.png",plot_alpha_divz_sgnsg[[2]],width = 14,height = 14)
```


## Composition

donuts plot without singletons fur 16s
```{r}
plots_comp <- NULL
for( i in c("Order_conf_rdp","Genus_conf_rdp")){
    tmp16s <- clone(meco_16s_ns)
    tmp16s$sample_table %<>% mutate(dumb_group = "sample")
    tmp16s$otu_table %<>% filter(rowSums(.)>1)
    tmp16s$tidy_dataset()
    
    tmp16s$cal_abund()
    tmp16s1 <- trans_abund$new(tmp16s,taxrank = i,ntaxa = 8,groupmean = "dumb_group")
    
    plot_data1 <- tmp16s1$data_abund
    use_taxanames <- tmp16s1$data_taxanames
    plot_data1$Taxonomy[!plot_data1$Taxonomy %in% c(use_taxanames,"unidentified")] <- "Others"
    plot_data1 %<>% 
        dplyr::group_by(!!!syms(c("Taxonomy","Sample"))) %>% 
        dplyr::summarise(Abundance = sum(Abundance)) %>% 
        as.data.frame(stringsAsFactors = FALSE)
    plot_data1$Taxonomy %<>% factor(., levels = c(use_taxanames[-9], "Others","unidentified"))
    plot_data1$label <- paste0(round(plot_data1$Abundance, 1),"%")
    donut_comp <- ggdonutchart(plot_data1,"Abundance",
                               fill="Taxonomy",
                               label = "label",
                               color = "white",
                               palette =  c(colorRampPalette(brewer.pal(8, "Dark2"))(9),"lightgrey"),
                               guide = guide_legend(reverse = TRUE) )+
        theme(legend.position = "right")

    tmp16s2 <- trans_abund$new(tmp16s,taxrank = i,ntaxa = 8)
    plot_data2 <- tmp16s2$data_abund
    plot_data2$Taxonomy[!plot_data2$Taxonomy %in% c(use_taxanames,"unidentified")] <- "Others"
    plot_data2$Taxonomy %<>% factor(., levels = rev(c(use_taxanames[-9], "Others","unidentified")))
   bar_comp <- plot_data2%>%
        ggplot(aes(x=Sample,y=Abundance,fill=Taxonomy),color=NA)+
        geom_bar(stat = "identity",width=1,position='fill')+
        scale_fill_manual(values =  rev(c(colorRampPalette(brewer.pal(8, "Dark2"))(9),"lightgrey")),
                          guide = guide_legend(reverse = TRUE) )+
        scale_y_continuous(expand = c(0,0))+
        theme(axis.text.x = element_blank(),
              axis.ticks.x = element_blank())
    
    plots_comp[[i]] <- donut_comp + bar_comp  +  plot_annotation(title = 'No singletons') & theme(plot.title = element_text(face='bold'))
}
plots_comp$Order_conf_rdp
```


```{r}
ggsave("Figures/comp_order.png",plots_comp$Order_conf_rdp,width = 14)
ggsave("Figures/com_genus.png",plots_comp$Genus_conf_rdp,width = 14)
```

```{r}
df <- meco_16s_ns$otu_table
df <- melt(as.matrix(df))
colnames(df) <- c("ASVs","Samples","Reads")
ord <- hclust(vegan::vegdist(labdsv::hellinger(t(meco_16s_ns$otu_table)),method = "horn"),method = "ward.D2")$order

df%>%
    mutate(Samples = gsub("[A-Z]|-|(?=_).*","",Samples,perl = T))%>%
    mutate(sample_ord = factor(Samples,
                               levels = gsub("[A-Z]|-|(?=_).*","",colnames(meco_16s_ns$otu_table),perl = T)[ord]))%>%
    ggplot(aes(x=ASVs,y=sample_ord,fill=log(Reads)))+
    geom_tile()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())
```


Distribution and prev/relab

```{r}
prev <- rowSums(meco_16s_ns$otu_table>0)
a <- prev%>%
    as.data.frame()%>%
ggplot(aes(x=prev))+
        geom_histogram(bins=30,color='darkcyan',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()+
    xlab("Cluster prevalence across samples")+
  coord_trans(y = "log1p")

median_relab <- NULL
mean_relab <- NULL
for(i in 1:nrow(meco_16s_ns$otu_table)){
    median_relab <- c(median_relab,median(as.numeric(meco_16s_ns$otu_table[i,]/colSums(meco_16s_ns$otu_table)))*100)
    mean_relab <- c(mean_relab,mean(as.numeric(meco_16s_ns$otu_table[i,]/colSums(meco_16s_ns$otu_table)))*100)
}
b <- median_relab%>%
    as.data.frame()%>%
ggplot(aes(x=median_relab))+
        geom_histogram(bins=30,color='burlywood3',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()+
    xlab("Cluster median relative abundance within samples (%)")+
  coord_trans(y = "log1p")

c <- mean_relab%>%
    as.data.frame()%>%
ggplot(aes(x=mean_relab))+
        geom_histogram(bins=30,color='darkolivegreen4',fill='white')+
        scale_y_continuous(expand = c(0,0))+
        theme_classic2()+
    xlab("Cluster mean relative abundance within samples (%)")+
  coord_trans(y = "log1p")

ggsave(filename = "Figures/prev_otu.png",plot=(a+b)/(c+plot_spacer()))
```


```{r}
ordi16s <- ordinate(file2meco::meco2phyloseq(meco_16s_ns), "NMDS", "horn")

tmp16s <- clone(meco_16s_ns)
tmp16s$sample_table %<>% mutate(dumb_group = "sample")
tmp16s$otu_table %<>% filter(rowSums(.)>1)
tmp16s$tidy_dataset()

tmp16s <- trans_abund$new(tmp16s,taxrank = "Genus",ntaxa = 8)

df_ordi_16s  <- merge(ordi16s$species,meco_16s_ns$tax_table,by = 'row.names')%>%
    mutate(Genus=ifelse(Genus%in%paste0("g__",tmp16s$data_taxanames),Genus,"Other"))%>%
    mutate(Genus=gsub('g__','',Genus))%>%
    mutate(Genus=as.factor(Genus))%>%
    mutate(Genus=fct_relevel(Genus,c(tmp16s$data_taxanames,"Other")))

ggplot(df_ordi_16s,aes(x=MDS1,y=MDS2,color=Genus,alpha=Genus))+
    geom_point(shape=16)+
    theme_classic()+
    ylab(paste0("NMDS 2"))+
    xlab(paste0("NMDS 1"))+
    geom_vline(xintercept=0,lty=2)+
    geom_hline(yintercept=0,lty=2)+
    scale_color_manual(values=colorRampPalette(brewer.pal(8, "Dark2"))(9))+
    scale_alpha_manual(values=c(rep(1,8),.5))

df_ordi_16s_sites <- ordi16s$points
df_ordi_16s_sites %<>%
  as.data.frame()%>%
  mutate(Sample=gsub("[A-Z]|-|(?=_).*","",rownames(.),perl = T))%>%
  left_join(combined)
ggplot(df_ordi_16s_sites,aes(x=MDS1,y=MDS2,label = Sample,color=Country))+
  # geom_point()+
  theme_dark()+
  geom_text()+
  scale_color_manual(values=c('#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000'))
  
```

```{r}
tmp16s <- clone(meco_16s_ns)
tmp16s$sample_table %<>% mutate(dumb_group = "sample")
tmp16s$otu_table %<>% filter(rowSums(.)>1)
tmp16s$tidy_dataset()

tmp16s$cal_abund()
tmp16s1 <- trans_abund$new(tmp16s,taxrank = "Order_conf_rdp",ntaxa = 8,groupmean = "dumb_group")

tmp16s2 <- trans_abund$new(tmp16s,taxrank = "Order_conf_rdp",ntaxa = 8)
plot_data2 <- tmp16s2$data_abund
use_taxanames <- tmp16s1$data_taxanames
plot_data2$Taxonomy[!plot_data2$Taxonomy %in% c(use_taxanames,"unidentified")] <- "Others"
plot_data2$Taxonomy %<>% factor(., levels = rev(c(use_taxanames[-9], "Others","unidentified")))
plot_data2%>%
  ggplot(aes(x=Sample_replace,y=Abundance,fill=Taxonomy),color=NA)+
  geom_bar(stat = "identity",width=1,position='fill')+
  scale_fill_manual(values =  rev(c(colorRampPalette(brewer.pal(8, "Dark2"))(9),"lightgrey")),
                    guide = guide_legend(reverse = TRUE) )+
  scale_y_continuous(expand = c(0,0))+
  theme(axis.text.x = element_text(angle = 45),
        axis.ticks.x = element_blank())+
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
```


Still some highly suspicious ASVs.


# Sites

We collated data from Google Earth Engine (GEE) using sample coordinate provided by our collaborators.
These data are from different datasets and satellite sources available on  [GEE website](https://developers.google.com/earth-engine/datasets).

Temporal data (Terra Climate, SMAP soil, Vegetation, Productivity) are average over years  and tagged "_lt" for "long-term".
In addition to these long term data we retrieved monthly data from march (_03) to october (_10) of the sampling year (2021).
These will be used to produce "short-term" (_st) data with the hypothesis that microbial communities are strongly affected by recent meteorology.

For each variable is given: name - unit - scale - resolution.


## Available Data

### Terra Climate

These are monthly weather data.
Long term : 2016/01/01 to 2021/01/01

With scale being a scaling factor by which to multiply the variable to get the right values.

**aet**: actual evapotranspiration - mm - 0.1 - 4638.3m
**def**: Climate water deficit - mm - 0.1 - 4638.3m
**pdsi**: Palmer drought severity index (temperature and precipitation -10 to +10 : dry to wet) - no unit - 0.01 - 4638.3m  
**pet**: Reference evapotranspiration - mm - 0.1 - 4638.3m  
**pr**: Precipitation accumulation (@ the end of the month) - mm - no scale - 4638.3m  
**srad**: Downward surface shortwave radiation - W/m^2 - 0.1 - 4638.3m  
**swe**: Snow water equivalent - mm - no scale - 4638.3m  
**tmmn**: Minimum Temperature - Â°C - 0.1 - 4638.3m  
**tmmx**: Maximum Temperature - Â°C - 0.1 - 4638.3m  
**vap**: Vapor pressure - KPa - 0.001 - 4638.3m  
**vpd**: Vapor pressure deficit - kPa - 0.01 - 4638.3m  
**moist**: Soil moisture - mm - 0.1 - 4638.3m  

### SMAP soil

NASA/SMAP/SPL3SMP_E/005

These are monthly soil data.
Long term : 2016/01/01 to 2021/01/01

**soil_moisture**: Top layer soil moisture (0 - 5) - volume fraction - no scale - 9000 m


### Topography

USGS/GMTED2010_FULL

**elevation**: Min elevation - m - no scale - 231.92m

### Vegetation

MODIS/006/MOD13Q1
MODIS/006/MCD15A3H

Long term : 2016/01/01 to 2021/01/01

**ndvi**: Normalized difference vegetation index over 16days - no unit - 0.0001 - 250m
**evi**: Enhanced vegetation index over 16days - no unit - 0.0001 - 250m
**lai**: Leaf area index over 4days - no unit - 0.1 - 500m
**fpar**: Fraction of absorbed photosynthecatilly active radiation (400-700nm) absorbed by the green elements of a vegetation canopy over 4days - no unit - 0.001 - 500m

### Productivity

MODIS/006/MOD17A2H

Long term : 2016/01/01 to 2021/01/01

**gpp**: Gross primary production 8days - kg*C/m^2 - 0.0001 - 500m
**npp**: Net primary production 8days - kg*C/m^2 - 0.0001 - 500m

### Open Land Map Soil

OpenLandMap/SOL/SOL_BULKDENS-FINEEARTH_USDA-4A1H_M/v02
OpenLandMap/SOL/SOL_CLAY-WFRACTION_USDA-3A1A1A_M/v02
OpenLandMap/SOL/SOL_ORGANIC-CARBON_USDA-6A1C_M/v02
OpenLandMap/SOL/SOL_PH-H2O_USDA-4C1A2A_M/v02
OpenLandMap/SOL/SOL_WATERCONTENT-33KPA_USDA-4B1C_M/v01

**bulk**: Bulk density @ 0 cm depth - kg/m^3 - 10 - 250m
**clay**: Clay content @ 0 cm depth - % (kg / kg) - no scale - 250m
**soc**: Organic carbon content @0 cm depth - g/kg - 5 - 250m
**ph**: pH in H20 @ 0 cm depth - pH - 10 - 250m
**water**: Water content @ field capacity (33kPa) @ 0 cm depth - % - no scale - 250m

### Human variables: Global Human Modification

CSP/HM/GlobalHumanModification

**ghm**: Global human modification index - fraction of a km^2 - no scale - 1000m


## Map Sites

```{r}
world <- ne_countries(scale = "medium", returnclass = "sf")

# Subset the world data to include only the specified latitude range
cropping <- as(raster::extent(-180, 180, 20, 90), "SpatialPolygons")


world_subset <- raster::crop(as_Spatial(world),cropping)
# Create a Lambert azimuthal equal-area projection centered on the North Pole
lambert_proj <- st_crs("+proj=laea +lat_0=90 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs")
world_lambert <- st_transform(st_as_sf(world_subset), crs = lambert_proj)


(sites <- st_as_sf(meco_16s_ns$sample_table, coords = c("X", "Y"), 
                   crs = 4326, agr = "constant"))

# Plot the map
ggplot() +
    geom_sf(data = world_lambert, fill = "grey", color = "darkgrey", size = 0.1)+
    geom_sf(data = sites, size = 3, shape = 23, fill = "darkred",alpha=.7) +
    ggtitle("MAPP 16s sites") +
    theme(plot.title = element_text(hjust = 0.5, size = 16))+
    theme_minimal()+
    theme(panel.background = element_rect(fill = "azure"))
```


## Select sites & deal with NAs

```{r}
site_data_raw <- read.csv("../../ressource/Site_edaphic_data/MappPtsSampled.csv")
site_data_raw %<>%
    mutate(Sample=ifelse(nchar(Sample)<3,paste0("0",Sample),Sample))%>%
    mutate(Sample=ifelse(nchar(Sample)<3,paste0("0",Sample),Sample))%>%
    mutate(Sample=ifelse(grepl("^8",Sample),paste0("0",Sample),Sample))%>% # fix 87a and 87b sample ids
    arrange(Sample)# fix sample ID to match the format of other MAPP datasets

site_data <- site_data_raw %>% filter(Sample%in%meco_16s_ns$sample_table$Sample)%>% # replace the NA placeholder -9999 with NAs
    mutate(across(.cols= colnames(site_data_raw),.fns = ~replace(., . ==  -9999 , NA)))
```


### Count NAs and Imputations


```{r}
site_data %<>% # remove short term variable of wrong months from site data
    select(names(site_data)[!grepl('06|07|08|09|10', names(site_data))])%>%
    rename(soil_moisture_04=soil_moisture_03_1)

var_missing_val <- as.data.frame(colSums(is.na(site_data)))%>% # plot NAs counts and %of variables
    rename(nb_na=`colSums(is.na(site_data))`)%>%
    mutate(var=rownames(.))%>%
    filter(nb_na!=0)%>%
    ggplot(aes(x=var,y=nb_na))+
    geom_bar(stat='identity',color='midnightblue',fill="white",width=1)+
    theme_classic2()+
    theme(axis.text.x = element_text(angle=45,hjust=1),
          plot.title = element_text(face="bold"))+
    scale_y_continuous(expand = c(0,0))+
    ggtitle("Number of missing values")+
    as.data.frame(colSums(is.na(site_data)))%>%
    rename(nb_na=`colSums(is.na(site_data))`)%>%
    mutate(var=rownames(.),
           na_pct=(nb_na/202)*100)%>%
    filter(nb_na!=0)%>%
    mutate(non_na_pct=100-na_pct)%>%
    reshape2::melt(id.variables=c(var,nb_na))%>%
    filter(variable!="nb_na")%>%
    mutate(variable=fct_relevel(variable,"non_na_pct","na_pct"))%>%
    ggplot(aes(x=var,y=value,fill=variable))+
    geom_bar(stat='identity',width=1,position="fill")+
    scale_fill_manual(values=c("lightgrey","mediumvioletred"))+
    theme_classic2()+
    theme(axis.text.x = element_text(angle=45,hjust=1),
          plot.title = element_text(face="bold"))+
    ggtitle("Percentage of missing values")+
    scale_y_continuous(expand = c(0,0))

ggsave("Figures/var_mssing_val.png",var_missing_val,width=16)
```

```{r}
as.data.frame(rowSums(is.na(site_data)))%>%
    ggplot(aes(x=1:nrow(.),y=`rowSums(is.na(site_data))`))+
    geom_bar(stat='identity',width=1)

site_data[which.max((rowSums(is.na(site_data)))),]
```


### Create short term

```{r}
var_lt <- colnames(site_data)[grep('_lt', names(site_data))] # extract long term variables (as they also exist on monthly basis)
months_we_want <- c("03","04","05") # march/april/may  (1 to 4 months before sampling)
var_st <- colnames(site_data)[grep('03|04|05', names(site_data))] # get columns of monthly variables

site_data_lt <- site_data%>% # subset long term variables
    select(all_of(c("X","Y",var_lt,"bulk","gHM","height","ph","soc","water")))
site_data_st <- site_data%>% # subset chort term variables
    select(all_of(c("X","Y",var_st,"bulk","gHM","height","ph","soc","water")))
```

### Impute missing data

We perform missing data imputation using missMDA package.

```{r} 
# Get the number of PCs we need to infer our missing values
ncp_data_lt <- site_data %>%  
    select(all_of(c("X","Y",var_lt,"bulk","gHM","height","ph","soc","water"))) %>%
    missMDA::estim_ncpPCA(scale=T)

ncp_data_st <- site_data %>% 
    select(all_of(c("X","Y",var_st,var_lt,"bulk","gHM","height","ph","soc","water"))) %>%
    mutate(across(everything(),as.numeric))%>%
    missMDA::estim_ncpPCA(scale=T)

# Impute our missing values and store them in a new 'imputed' data frame
imputed_data_lt <- site_data %>% 
    select(all_of(c("X","Y",var_lt,"bulk","gHM","height","ph","soc","water")))%>%
    missMDA::imputePCA(ncp=5,scale=T)
imputed_data_lt <- imputed_data_lt$completeObs %>% as.data.frame()

imputed_data_st <- site_data %>% 
    select(all_of(c("X","Y",var_st,var_lt,"bulk","gHM","height","ph","soc","water")))%>%
    missMDA::imputePCA(ncp=5,scale=T)
imputed_data_st <- imputed_data_st$completeObs %>% as.data.frame()
```

Our short term data are averaged over three months (march to may).
We need to test if imputed before or on mean changes a lot to imputed values.

```{r}
# Test by averaging short term variables before imputation
site_data_st_before_imputation <- site_data %>% 
    select(all_of(c("X","Y",var_lt,var_st,"bulk","gHM","height","ph","soc","water")))%>%
    mutate(across(.cols=all_of(var_lt),
                  ~get(paste0(gsub("_lt","_",cur_column()), months_we_want[1]))+
                       get(paste0(gsub("_lt","_",cur_column()), months_we_want[2]))+
                       get(paste0(gsub("_lt","_",cur_column()), months_we_want[3]))/3,
                  .names = "{col}__st"
    ))

var_to_remove <- colnames(site_data_st_before_imputation)[grep('_[0-9]{2}', names(site_data_st_before_imputation))]
site_data_st_before_imputation %<>%
    select(-one_of(var_to_remove))%>% 
    rename_with(~ stringr::str_remove(., "_lt_"), everything())

st <- names(site_data_st_before_imputation)[grep('_st', names(site_data_st_before_imputation))]
ncp_data_before_st <- site_data_st_before_imputation %>% 
    select(all_of(c("X","Y",st,"bulk","gHM","height","ph","soc","water"))) %>%
    mutate(across(everything(),as.numeric))%>%
    missMDA::estim_ncpPCA(scale=T)


site_data_st_before_imputation_imputed <- site_data_st_before_imputation %>% 
    select(all_of(c("X","Y",st,"bulk","gHM","height","ph","soc","water")))%>%
    missMDA::imputePCA(ncp=3,scale=T)
site_data_st_before_imputation_imputed <- site_data_st_before_imputation_imputed$completeObs %>% as.data.frame()

# Here we average on imputed variables
site_data_st_after_imputation <- imputed_data_st %>%
    mutate(across(.cols=all_of(var_lt),
                  ~get(paste0(gsub("_lt","_",cur_column()), months_we_want[1]))+
                       get(paste0(gsub("_lt","_",cur_column()), months_we_want[2]))+
                       get(paste0(gsub("_lt","_",cur_column()), months_we_want[3]))/3,
                  .names = "{col}__st"
    ))

var_to_remove <- colnames(site_data_st_after_imputation)[grep('_[0-9]{2}', names(site_data_st_after_imputation))]
site_data_st_after_imputation %<>%
    select(-one_of(var_to_remove))%>% 
    rename_with(~ stringr::str_remove(., "_lt_"), everything())%>%
    select(all_of(c("X","Y",st,"bulk","gHM","height","ph","soc","water")))
```

Test multiple imputation
```{r}
# test <- site_data_st_before_imputation %>% 
#     select(all_of(c("X","Y",st,"bulk","gHM","height","ph","soc","water")))%>%
#     missMDA::MIPCA(ncp=3,scale=T,nboot = 1000)
# 
# plot(test)
```
```{r}
# test2 <- site_data %>% 
#     select(all_of(c("X","Y",var_st,var_lt,"bulk","gHM","height","ph","soc","water")))%>%
#     missMDA::MIPCA(ncp=3,scale=T,nboot = 1000)
# 
# plot(test2)
```


## Select variables

### Histogram of variables


```{r}
data_sets <- c("site_data_lt",
               "site_data_st",
               'imputed_data_lt',
               'imputed_data_st',
               "site_data_st_after_imputation",
               "site_data_st_before_imputation_imputed")

for(j in data_sets){
    message(j)
    tmp <- get(j)
    plot_list <- NULL
    for(i in 1:ncol(tmp)){
        plot_list[[names(tmp[i])]] <- local({
            i <- i
            ggplot(tmp,aes(x=tmp[,i]))+
            geom_histogram(fill="darkolivegreen3")+
            theme_classic2()+
            xlab(names(tmp[i]))})
        
    }
    ggsave(plot=ggarrange(plotlist =  plot_list,
                          ncol = ceiling(sqrt(i)),
                          nrow = ceiling(sqrt(i))),
                          filename=paste0("Figures/histograms/",j,".png"),
                          height =25,
                          width = 25)
}
```

### Var correlation

```{r}
data_sets <- c("site_data_lt",
               'imputed_data_lt',
               "site_data_st_after_imputation",
               "site_data_st_before_imputation_imputed")

for(j in data_sets){
    message(j)
    tmp <- get(j)
    names(tmp) <- gsub("_.t$","",names(tmp))
    pairs <- ggpairs(tmp,
                     columns=3:ncol(tmp),
                     progress = F,
                     axisLabels = "none",
                     # LOWER TRIANGLE ELEMENTS: add line with smoothing; make points transparent and smaller
                     lower = list(continuous = function(...) 
                         ggally_smooth(..., colour="darkolivegreen3", alpha = 0.3, size=0.8)), 
                     # DIAGONAL ELEMENTS: histograms
                     diag = list(continuous = function(...) 
                         ggally_barDiag(..., fill="grey")),
                     
                     # UPPER TRIANGLE ELEMENTS: use fct. creating corr heatmap with sign stars
                     upper = list(continuous = cor_fun)) + 
        theme(strip.background = element_blank(), # remove color
              strip.text = element_text(size=6,angle=45), # change font and font size
              axis.line = element_line(colour = "grey"),
              # remove grid
              panel.grid.minor = element_blank(), )+  # remove smaller gridlines
        # panel.grid.major = element_blank()    # remove larger gridlines)
        ggtitle(j)+
        theme(plot.title = element_text(face="bold"))
    
    
    ggsave(plot=pairs,
           filename=paste0("Figures/pairs_",j,".png"),
           height =29.7/2.54,
           width = 21/2.54)
}
```

Soil moisture imputation strategies changes its relations with other variables: REMOVE IT.


### PCA sites

```{r}
pca_sites_lt <- imputed_data_lt %>% 
    select(!grep('_st', names(imputed_data_lt)))%>%
    select(!grep('soil_moisture',colnames(.)))%>%
    select(-c(X,Y))%>%
    FactoMineR::PCA(scale.unit = T,
                    graph = F)

pca_sites_before_st <- site_data_st_before_imputation_imputed %>% 
    select(grep('_st', names(site_data_st_before_imputation_imputed)))%>%
    select(!grep('soil_moisture',colnames(.)))%>%
    FactoMineR::PCA(scale.unit = T,
                    graph = F) # NAs

pca_sites_after_st <- site_data_st_after_imputation %>% 
    select(grep('_st', names(site_data_st_after_imputation)))%>%
    select(!grep('soil_moisture',colnames(.)))%>%
    FactoMineR::PCA(scale.unit = T,
                    graph = F)
```

```{r}
data_sets <- c("pca_sites_lt","pca_sites_before_st","pca_sites_after_st")
col <- c("darkorange","cornflowerblue","darkorchid4")
pca_site_biplot <- NULL
scree_plot_list <- NULL

for(i in data_sets){
    pca_tmp <- get(i)
    
    eig <- pca_tmp$eig
    coord_pca <- as.data.frame(pca_tmp$ind$coord)
    coord_pca$site <- site_data$Sample
    coord_pca$country <- site_data$Country
    
    mul <- arrowMul(as.data.frame(pca_tmp$var$coord),
                    pca_tmp$ind$coord)
    
    pca_site_biplot[[i]] <- ggplot()+
        geom_point(coord_pca,mapping=aes(x=Dim.1,y=Dim.2),color=col[match(i,data_sets)])+
        theme_classic2()+
        geom_vline(xintercept = 0,lty=2)+
        geom_hline(yintercept = 0,lty=2)+
        # ggrepel::geom_label_repel(data=filter(coord_pca,Dim.2< -3),aes(x=Dim.1,y=Dim.2,label=site))+
        xlab(paste0("PC1 (",round(eig[1,2],2),"%)"))+
        ylab(paste0("PC2 (",round(eig[2,2],2),"%)"))+
        geom_segment(data= as.data.frame(pca_tmp$var$coord),aes(x = 0, y = 0, xend=mul*Dim.1, yend=mul*Dim.2),
                     lineend = "round", 
                     linejoin = "round",
                     linewidth = .75, 
                     arrow = arrow(length = unit(0.2, "inches")),
                     colour = "black" 
        )+
        ggrepel::geom_text_repel(data = data.frame(var=rownames(pca_tmp$var$coord),pca_tmp$var$coord*mul), # add variable names at the end of arrows
                                 aes(x = ifelse(Dim.1<0,Dim.1*1.1,Dim.1*1.1), # nudge a bit the coordinates so that they're not on the arrows
                                     y = ifelse(Dim.2<0,Dim.2*1.1,Dim.2*1.1),
                                     label = var),
                                 max.overlaps = getOption("ggrepel.max.overlaps", default = 15))+
        ggtitle(paste0("Biplot sites ",i))+
        theme(plot.title = element_text(face="bold"))
    
    scree_plot_list[[i]] <- factoextra::fviz_screeplot(pca_tmp,barfill = col[match(i,data_sets)], barcolor=col[match(i,data_sets)])
}
```

```{r}
pca_sites <- (pca_site_biplot$pca_sites_lt+scree_plot_list$pca_sites_lt)/
    (pca_site_biplot$pca_sites_before_st+scree_plot_list$pca_sites_before_st)/
    (pca_site_biplot$pca_sites_after_st+scree_plot_list$pca_sites_after_st)
```

```{r}
ggsave("Figures/pca_sites.png",pca_sites,width = 21/2.54 ,height= 29.7/2.54)
```

# Save cleaned datasets

I need meco_16s_ns which is our communities with no singletons.
I need site data that are imputed.
I'll go with data imputed after short term averaging to reduce noise.
Selected variables!!

```{r}
site_data_st_before_imputation_imputed$Sample <- site_data$Sample
imputed_data_lt$Sample <- site_data$Sample
combined <- site_data_st_before_imputation_imputed%>%
    left_join(imputed_data_lt, by="Sample", suffix=c("",".y"))%>%
    select(-ends_with(".y"))%>%
    left_join(site_data[,1:4])%>%
    relocate(Sample)


# save.image(file='outputs/preproc_myenv.RData')
save(meco_16s_ns,file = "outputs/meco_16s.RData")
save(combined,file = "outputs/site_data_imputed.RDS")
save(non_targ_asvs, file = "non_targ_asvs.rds")
rm(list=ls())
gc()
```

```{r}
load("outputs/meco_16s.RData")
load("outputs/site_data_imputed.RDS")
```

```{r}
country_color <- c('#e6194b',
                   '#3cb44b',
                   '#ffe119',
                   '#4363d8',
                   '#f58231', 
                   '#911eb4', 
                   '#46f0f0',
                   '#f032e6', 
                   '#bcf60c',
                   '#fabebe', 
                   '#008080', 
                   '#e6beff', 
                   '#9a6324',
                   '#fffac8',
                   '#800000', 
                   '#aaffc3',
                   '#808000',
                   '#ffd8b1', 
                   '#000075',
                   '#808080', 
                   '#ffffff',
                   '#000000')
```


# 16s and sites relations

## Alpha div patterns

```{r}
meco_16s_ns$sample_table%>%
  ggplot(aes(x=X,y=nb_asvs))+
  geom_point()+
  theme_classic2()+
  labs(x="Latitude",y="Richness")+
meco_16s_ns$sample_table%>%
  ggplot(aes(x=Y,y=nb_asvs))+
  geom_point()+
  theme_classic2()+
  labs(x="Longitude",y="Richness")
meco_16s_ns$sample_table%<>%left_join(combined)

meco_16s_ns$sample_table%>%
  ggplot(aes(x=Country,y=nb_asvs,color=Country))+
  geom_jitter()+
  scale_color_manual(values=country_color)+
  theme_classic2()+
  labs(x="Country",y="Richness")+
  theme(axis.text.x = element_text(angle = 90))


world <- ne_countries(scale = "medium", returnclass = "sf")

# Subset the world data to include only the specified latitude range
cropping <- as(raster::extent(-180, 180, 28, 90), "SpatialPolygons")


world_subset <- raster::crop(as_Spatial(world),cropping)
# Create a Lambert azimuthal equal-area projection centered on the North Pole
lambert_proj <- st_crs("+proj=laea +lat_0=90 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs")
world_lambert <- st_transform(st_as_sf(world_subset), crs = lambert_proj)


(sites <- st_as_sf(meco_16s_ns$sample_table, coords = c("X", "Y"), 
                   crs = 4326, agr = "constant"))

# Plot the map
ggplot() +
  geom_sf(data = world_lambert, fill = "grey", color = "darkgrey", size = 0.1)+
  geom_sf(data = sites, size = 3, shape = 23, aes(fill = nb_asvs),alpha=.7) +
  ggtitle("MAPP 16s sites") +
  theme(plot.title = element_text(hjust = 0.5, size = 16))+
  theme_minimal()+
  theme(panel.background = element_rect(fill = "azure"))+
  scale_fill_viridis_c(limits = c(150, 1900), oob = scales::squish)
```


## LMM

```{r}
var_lt <- colnames(combined)[which(grepl("lt",colnames(combined)))]
var_st <- colnames(combined)[which(grepl("st",colnames(combined)))]
var_interest <- c("aet","pr","swe","tmmn","tmmx","vap","vpd","moist","height","evi","fpar","gpp","gHM","ph","water","soc")
var_lt <- var_lt[which(gsub("_lt","",var_lt)%in%var_interest)]
var_st <- var_st[which(gsub("_st","",var_st)%in%var_interest)]

XFormula_lt <- paste(var_lt,sep="", collapse = "+")
XFormula_st <- paste(var_st,sep="", collapse = "+")

XFormula_lt <- paste0("nb_asvs ~ ph + height + water + gHM + soc +", XFormula_lt ,"+ nb_reads ")
XFormula_st <- paste0("nb_asvs ~ ph + height + water + gHM + soc +", XFormula_st ,"+ nb_reads ")

combined %<>% left_join(meco_16s_ns$sample_table)

test <- lm(formula = XFormula_st,
                    data = combined)
anova(test)
```

## dbRDA

```{r}
arrowMul <- function(arrows, data, at = c(0, 0), fill = 0.75) {
  u <- c(range(data[,1], range(data[,2])))
  u <- u - rep(at, each = 2)
  r <- c(range(arrows[, 1], na.rm = TRUE), range(arrows[, 2], na.rm = TRUE))
  rev <- sign(diff(u))[-2]
  if (rev[1] < 0)
    u[1:2] <- u[2:1]
  if (rev[2] < 0)
    u[3:4] <- u[4:3]
  u <- u/r
  u <- u[is.finite(u) & u > 0]
  fill * min(u)
}


formulaZ <- c(XFormula_lt,XFormula_st)
dbrda_site_biplot <- NULL

for(i in formulaZ){
    
    formula_tmp <- paste0("labdsv::hellinger(t(meco_16s_ns$otu_table))~Condition(nb_reads)+",gsub("\\+ nb_reads.*$|nb_asvs ~","",i))
    dbrda_tmp <- dbrda(formula=formula(formula_tmp),data=combined, distance="horn") # full model with morisita horn distance based on hellinger transformed data
    
    #get eigen values of axes
    eig <- c(dbrda_tmp$CCA$eig)
    eig1 <- round((eig[1]/sum(eig))*100,digits = 2) # % of explained inertia of the first axis
    eig2 <- round((eig[2]/sum(eig))*100,digits = 2) # % of explained inertia of the second axis
    
    fort_rda_tmp <- fortify(dbrda_tmp) # transform dbrda results in a ggplot2 usable format
    vars <- c("dbRDA1","dbRDA2") # we will plot in the first two dimensions
    want <- fort_rda_tmp[["score"]] == "biplot" # get variables scores (arrows)
    mul <- arrowMul(fort_rda_tmp[want, vars, drop = FALSE],
                    fort_rda_tmp[!want, vars, drop = FALSE]) # get the scaling factor to scale these scores for plotting as in ggvegan
    fort_rda_tmp[want, vars] <- mul * fort_rda_tmp[want, vars] # scale the scores 
    
    
    dbrda_site_biplot[[formula_tmp]] <- ggplot(fort_rda_tmp, aes(x = dbRDA1, y = dbRDA2)) +
        geom_point(data = cbind(subset(fort_rda_tmp, score == "sites"), # add points (sites) corresponding to samples
                                country=combined$Country), 
                   aes(color = country), 
                   size = 3) +
        geom_segment(data = subset(fort_rda_tmp, score == "biplot"), # add arrows for variables
                     aes(x = 0, y = 0, xend=dbRDA1, yend=dbRDA2),
                     lineend = "round", 
                     linejoin = "round",
                     linewidth = .75, 
                     arrow = arrow(length = unit(0.2, "inches")),
                     colour = "black" ) +
        coord_fixed()+ # fix proportion between axis units
        geom_text(data = fort_rda_tmp[want, , drop = FALSE ], # add variable names at the end of arrows
                  aes(x = ifelse(dbRDA1<0,dbRDA1*1.2,dbRDA1*1.2), # nudge a bit the coordinates so that they're not on the arrows
                      y = ifelse(dbRDA2<0,dbRDA2*1.2,dbRDA2*1.2),
                      label = label))+
        theme_classic()+
        xlab(paste0("dbRDA1 (",eig1,"%)"))+
        ylab(paste0("dbRDA2 (",eig2,"%)"))+
        guides(fill="none")  +
        geom_hline(yintercept=0,linetype = "dashed")+
        geom_vline(xintercept=0,linetype = "dashed")+
        ggtitle(formula_tmp)+
        theme(plot.title = element_text(face="bold"))+
        coord_fixed()
}
```


```{r}
dbrda_site_biplot$`labdsv::hellinger(t(meco_16s_ns$otu_table))~Condition(nb_reads)+ ph + height + water + gHM + soc +aet_lt+moist_lt+pr_lt+swe_lt+tmmn_lt+tmmx_lt+vap_lt+vpd_lt+evi_lt+fpar_lt+gpp_lt`+
    dbrda_site_biplot$`labdsv::hellinger(t(meco_16s_ns$otu_table))~Condition(nb_reads)+ ph + height + water + gHM + soc +aet_st+moist_st+pr_st+swe_st+tmmn_st+tmmx_st+vap_st+vpd_st+evi_st+fpar_st+gpp_st`+
    plot_layout(guides="collect")
```


# Picrust2

## Prepare inputs

Create from R picrust2 inputs
Galaxy Picrust2 needs a .Biom and .Fasta file.

.Fasta -> sequences and Header
.Biom -> tougher to create, will create a .Tsv and convert to .Biom within Galaxy
```{r}
seqs <- as.list(gsub("s__","",meco_16s_ns$tax_table$sequence))
names_seqs <- rownames(meco_16s_ns$tax_table)
seqinr::write.fasta(seqs,names_seqs,"outputs/fasta4picrust2.fasta")
```

```{r}
data_temp <- read_delim(file = "frogs_input/Galaxy169-[FROGS_BIOM_to_TSV__abundance.tsv].tsv", # read the tsv file
                        delim = "\t", escape_double = FALSE, 
                        trim_ws = TRUE)
data_temp%<>%
    filter(observation_name%in%names_seqs)
write_delim(data_temp,
            file = "outputs/tsv4picrust.tsv",
            delim = "\t", )
```




## Alignment and stuff

```{r}
picrust_closestref <-  read_delim("picrust2/Galaxy228-[FROGSFUNC_1_placeseqs_and_copynumbers__frogsfunc_placeseqs_closests_ref_sequences.txt].tsv", # read the tsv file
                                  delim = "\t", escape_double = FALSE, 
                                  trim_ws = TRUE)
```

check ASVs discrepencies between Picrust2 and my cleaned dataset
```{r}
nrow(meco_16s_ns$tax_table) #12792 ASVs in my cleaned dataset
nrow(picrust_closestref) # 12792 ASVs in picrust

all(rownames(meco_16s_ns$tax_table) %in% picrust_closestref$`#ASV`)
```

```{r}
picrust_closestref %>%
    ggplot()+
    geom_point(aes(x=NSTI,y=`%cov`,color="darkorange"))+
    geom_point(aes(x=NSTI,y=`%id`,color="darkorchid4"))+
    theme_classic2()+
    ylab("Metric %") +
     scale_color_identity(name = "Blast metrics",
                          breaks = c("darkorange", "darkorchid4"),
                          labels = c("Coverage", "Identity"),guide = "legend")+ 
    scale_x_continuous(breaks = scales::breaks_width(2),expand = c(0, 0))+
    ggplot(picrust_closestref)+
    geom_point(aes(x=NSTI,y=score), color="aquamarine4")+
    theme_classic2()+
    ylab("Blast score")
```

Clusters with NSTI > 2 are considered poorly represented in the Picrust2 reference database.
Additionally, a coverage < 80% is pretty low 

```{r}
(picrust_closestref %>%
    ggplot(aes(x=score,y=`%id`))+
    geom_point()+
    picrust_closestref %>%
    ggplot(aes(x=`%id`,y=`%cov`))+
    geom_point()+
    picrust_closestref %>%
    ggplot(aes(x=score,y=`%cov`))+
    geom_point())/
    (picrust_closestref %>%
         filter(`%cov`>80&`%id`>80&NSTI<2)%>%
    ggplot(aes(x=score,y=`%id`))+
    geom_point()+
    picrust_closestref %>%
         filter(`%cov`>80&`%id`>80&NSTI<2)%>%
    ggplot(aes(x=`%id`,y=`%cov`))+
    geom_point()+
    picrust_closestref %>%
         filter(`%cov`>80&`%id`>80&NSTI<2)%>%
    ggplot(aes(x=score,y=`%cov`))+
    geom_point())
```

```{r}
picrust_closestref %>%
    filter(NSTI<2)%>%
    filter(`%cov`>80&`%id`>80)%>%
    nrow()/nrow(picrust_closestref) # 93% of OTUs

picrust_closestref %>%
    filter(NSTI<2)%>%
    filter(`%cov`>80&`%id`>80)%>%
    summarise(sum_seq=sum(`Nb sequences`))/sum(picrust_closestref$`Nb sequences`) # Keep 98% of sequences
```

I don't have much confidence in these ASVs associated predictions cuz poorly mapped in Picrust2 reference db so filter them out. 

```{r}
asvs_picrust <- picrust_closestref %>%
    filter(NSTI<2&`%cov`>80&`%id`>80)%>%
    select(`#ASV`)
discarded_picrust <- picrust_closestref %>%
         filter(!`#ASV`%in%asvs_picrust$`#ASV`)%>%select(`#ASV`)

asvs_picrust <- asvs_picrust$`#ASV` #12486 asvs
discarded_picrust <- discarded_picrust$`#ASV` # 306 asvs
```

Set cov and Id to 0.8 in galaxy FrogsFunc 2.

Investigate Excluded ASVs and their repartition across my samples.
306 ASVs should be excluded with this filtering

```{r}
meco_picrust_16s <- clone(meco_16s_ns)
meco_picrust_16s$tax_table%<>%
    mutate(is_picrust=ifelse(rownames(.)%in%asvs_picrust,"Yes","No"))%>%
    mutate(cluster=rownames(.))
meco_picrust_16s$otu_table%>%
    mutate(cluster=rownames(.))%>%
    reshape2::melt()%>%
    left_join(meco_picrust_16s$tax_table)%>%
    ggplot(aes(x=variable,y=value,fill=is_picrust))+
    geom_bar(stat="identity",width = 1)+
    theme(axis.text.x = element_blank())+
    ggtitle("Reads of picrust or not OTUs")+
    scale_y_continuous(expand = c(0,0))+
    meco_picrust_16s$otu_table%>%
   mutate(across(everything(), ~ .x/sum(.)))%>%
    mutate(cluster=rownames(.))%>%
    reshape2::melt()%>%
    left_join(meco_picrust_16s$tax_table)%>%
    ggplot(aes(x=variable,y=value,fill=is_picrust))+
    geom_bar(stat="identity",width = 1)+
    theme(axis.text.x = element_blank())+
    ggtitle("Reads proportion of picrust or not OTUs")+
    scale_y_continuous(expand = c(0,0))
```

```{r}
rownames(meco_16s_ns$sample_table) <- meco_16s_ns$sample_table$sample_id
meco_16s_picrust2 <- clone(meco_16s_ns)

meco_16s_picrust2$otu_table %<>% filter(rownames(.)%in%asvs_picrust)

meco_16s_picrust2$tidy_dataset()
```

```{r}
ab_trans_meco_16s_picrust2 <- trans_abund$new(meco_16s_picrust2,taxrank = "Order",ntaxa = 8)
order_relab_16s_picrust2 <- ab_trans_meco_16s_picrust2$plot_bar(others_color = "grey70", xtext_keep = FALSE, legend_text_italic = FALSE)+
    geom_col(width = 1)+
    ggtitle("MAPP 16s")+
    theme(plot.title = element_text(face='bold'))
order_relab_16s_picrust2
```

## Functions and pathways

We investigate functions and pathways using two distinct databases (Kegg and MetaCyc).
### Load and ID xcluded ASVs

```{r}
pathways_db <- c("Kegg","MetaCyc")
```


```{r}
kegg_files <- NULL
metacyc_files <- NULL
for(i in pathways_db){
    
    # get file from good database
    shrtct <- ifelse(i=="Kegg","KO","EC")
    
    # load function copy number predictions
    if(shrtct == "KO"){
      kegg_files[["picrust_fct_pred_copynb"]] <- read_delim("picrust2/Galaxy238-[FROGSFUNC_2_functions__KO_copynumbers_predicted.tsv].tsv",
                                              delim = "\t", escape_double = F, trim_ws = T)
      
      kegg_files[["picrust_fct_unstrat"]] <- read_delim("picrust2/Galaxy240-[FROGSFUNC_2_functions___frogsfunc_functions_unstrat_KO.tsv].tsv",
                                              delim = "\t", escape_double = F, trim_ws = T)
    }
    else{
       metacyc_files[["picrust_fct_pred_copynb"]] <- read_delim("picrust2/Galaxy237-[FROGSFUNC_2_functions__EC_copynumbers_predicted.tsv].tsv",
                                              delim = "\t", escape_double = F, trim_ws = T)
      
      metacyc_files[["picrust_fct_unstrat"]] <- read_delim("picrust2/Galaxy239-[FROGSFUNC_2_functions___frogsfunc_functions_unstrat_EC.tsv].tsv",
                                              delim = "\t", escape_double = F, trim_ws = T)
    }
    

}
    # Load asv excluded by galaxy FROGs
    galaxy_Xcluded <- read_delim("picrust2/Galaxy236-[FROGSFUNC_2_functions__frogsfunc_functions_asv_excluded.tsv].tsv",
                                              delim = "\t", escape_double = F, trim_ws = T)
    # load normalize marker abundance
    picrust_norm_marker <- read_delim("picrust2/Galaxy240-[FROGSFUNC_2_functions___frogsfunc_functions_unstrat_KO.tsv].tsv",
                                              delim = "\t", escape_double = F, trim_ws = T)
```


```{r}
length(discarded_picrust)
nrow(galaxy_Xcluded)
galaxy_Xcluded$`#Cluster` %in% discarded_picrust

setdiff(discarded_picrust,galaxy_Xcluded$`#Cluster`) 
```

```{r}
picrust_closestref%>%
  filter(`#ASV`%in%setdiff(discarded_picrust,galaxy_Xcluded$`#Cluster`)) #difference due to 80% ID. Frogs on Galaxy is likely conservative while I specified "Strictly Inferior to .8"
```

```{r}
asvs_picrust <- picrust_closestref %>%
    filter(NSTI<2&`%cov`>80&`%id`>=80)%>%
    select(`#ASV`)
discarded_picrust <- picrust_closestref %>%
         filter(!`#ASV`%in%asvs_picrust$`#ASV`)%>%select(`#ASV`)

asvs_picrust <- asvs_picrust$`#ASV` #12499 asvs
discarded_picrust <- discarded_picrust$`#ASV` #293 ASVs Now we match Galaxy's Xclusion
```


### Xplore functions

```{r}
amino_color <- c("#4e8937",
                 "#733cc9",
                 "#6dc140",
                 "#cf4cc3",
                 "#63b889",
                 "#d24172",
                 "#4ca3ae",
                 "#d74c33",
                 "#656ad1",
                 "#cf9133",
                 "#6d2f7d",
                 "#a6a34d",
                 "#bb86cb",
                 "#4a5c2f",
                 "#6f97d0",
                 "#814025",
                 "#434a7b",
                 "#cf8c70",
                 "#78334b",
                 "#d0849f")
```


```{r}
plot_fct <- NULL
plot_fct_tgt <- NULL
ordiplot_fct <- NULL
donutplot_fct <- NULL
for(i in pathways_db){
  if(i == "Kegg"){
    d_tmp <- kegg_files
  }else{
    d_tmp <- metacyc_files
  }
  names(d_tmp$picrust_fct_unstrat)[grepl("L001",names(d_tmp$picrust_fct_unstrat))] <- gsub("[A-Z]|-|(?=_).*","",names(d_tmp$picrust_fct_unstrat)[grepl("L001",names(d_tmp$picrust_fct_unstrat))] ,perl = T)
  
  print("% of NA")
  a <- d_tmp$picrust_fct_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    summarise(total = sum(value))
  b <- d_tmp$picrust_fct_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    group_by(c1,c2,c3,c4)%>%
    summarise(total = sum(value))%>%
    filter(is.na(c1))
  
  print((b$total/a$total)*100)
    
    
  plot_fct[[i]] <- d_tmp$picrust_fct_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c1))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values =RColorBrewer::brewer.pal(11,'Set3'))+
    ylab("Function %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
    d_tmp$picrust_fct_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c1))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values =RColorBrewer::brewer.pal(11,'Set3'))+
    ylab("Function %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))
  
  plot_fct_tgt[[i]] <- (d_tmp$picrust_fct_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    filter(grepl("glucosidase",classification))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c4))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values =RColorBrewer::brewer.pal(11,'Set3'))+
    ylab("Function %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
    d_tmp$picrust_fct_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    filter(grepl("aminopep",classification))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c4))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values = amino_color)+
    ylab("Enzymes")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
    plot_annotation(title = paste0(i,' Functions')))/
    (d_tmp$picrust_fct_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    filter(grepl("glucosidase",classification))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c4))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values =RColorBrewer::brewer.pal(11,'Set3'))+
    ylab("Function %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
    d_tmp$picrust_fct_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    filter(grepl("aminopep",classification))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c4))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values =amino_color)+
    ylab("Enzymes")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
    plot_annotation(title = paste0(i,' Functions')))
  
  
  ordi_fct <- d_tmp$picrust_fct_unstrat%>%
    select(c(which(names(.)%in%combined$Sample)))%>%
    t()%>%
    metaMDS()

  df_ordi_fct_sites <- ordi_fct$points
  df_ordi_fct_sites %<>%
    as.data.frame()%>%
    mutate(Sample=rownames(.))%>%
    left_join(combined)
  
  ordiplot_fct[[i]] <- ggplot(df_ordi_fct_sites,aes(x=MDS1,y=MDS2,label = Sample,color=Country))+
    geom_point()+
    theme_dark()+
    # geom_text()+
    scale_color_manual(values=country_color)

  df_donut <- d_tmp$picrust_fct_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    drop_na()%>%
    group_by(c1,c2,c3,c4)%>%
    summarise(abund = sum(value))%>%
    mutate(high_lvl=c1)%>%
    reshape2::melt(measure.vars = c("c1","c2","c3","c4"))%>%
    group_by(variable)%>%
    mutate(relab = (abund/sum(abund))*100)%>%
    group_by(high_lvl,variable,value)%>%
    summarise(relab_smr = sum(relab))
  
  
  donutplot_fct[[i]] <- df_donut %>%
    ggplot(aes(x=variable,y=relab_smr,fill=high_lvl))+
    geom_bar(stat = "identity",position = "stack",color='white',width=1)+
    scale_fill_manual(values =RColorBrewer::brewer.pal(11,'Set3'))+
    ylab("Function %")+
    xlab("Samples")+
    theme_void()+
    coord_polar(theta = 'y')+
    ggtitle("Overall functional diversity without NAs")
  
  
}
```

```{r}
kegg_plots <- (plot_fct$Kegg)/(donutplot_fct$Kegg+ordiplot_fct$Kegg)
metacyc_plots <- (plot_fct$MetaCyc)/(donutplot_fct$MetaCyc+ordiplot_fct$MetaCyc)

kegg_plots_tgt <- plot_fct_tgt$Kegg
metacyc_plots_tgt <- plot_fct_tgt$MetaCyc

ggsave("Figures/keggplot.png",kegg_plots,width = 10,height = 10)
ggsave("Figures/metacycplot.png",metacyc_plots,width = 10,height = 10)

ggsave("Figures/keggplottgt.png",kegg_plots_tgt,width = 10,height = 10)
ggsave("Figures/metacycplottgt.png",metacyc_plots_tgt,width = 10,height = 10)
```

### Xplore pathways

```{r}
kegg_files[["picrust_pathway_unstrat"]] <- read_delim("picrust2/Galaxy242-[FROGSFUNC_3_pathways__frogsfunc_pathways_unstrat.tsv].tsv",
                                                      delim = "\t", escape_double = F, trim_ws = T)

metacyc_files[["picrust_pathway_unstrat"]] <- read_delim("picrust2/Galaxy244-[FROGSFUNC_3_pathways__frogsfunc_pathways_unstrat.tsv].tsv",
                                                         delim = "\t", escape_double = F, trim_ws = T)
```


```{r}
names(kegg_files$picrust_pathway_unstrat)[grepl("L001",names(kegg_files$picrust_pathway_unstrat))] <- gsub("[A-Z]|-|(?=_).*","",names(kegg_files$picrust_pathway_unstrat)[grepl("L001",names(kegg_files$picrust_pathway_unstrat))] ,perl = T)

getPalette <- colorRampPalette(RColorBrewer::brewer.pal(12,'Set3'))

kegg_files$picrust_pathway_unstrat%>%
  select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c2))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values = getPalette(35))+
    ylab("Function %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))


names(metacyc_files$picrust_pathway_unstrat)[grepl("L001",names(metacyc_files$picrust_pathway_unstrat))] <- gsub("[A-Z]|-|(?=_).*","",names(metacyc_files$picrust_pathway_unstrat)[grepl("L001",names(metacyc_files$picrust_pathway_unstrat))] ,perl = T)

metacyc_files$picrust_pathway_unstrat%>%
  select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c2))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values = getPalette(37))+
    ylab("Function %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))

```

```{r}
metacyc_files$picrust_pathway_unstrat%>%
  select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c4))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values = getPalette(414))+
    ylab("Function %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
  theme(legend.position = "none")

```

```{r}
plot_pth <- NULL
ordiplot_pth <- NULL
for(i in pathways_db){
  if(i == "Kegg"){
    d_tmp <- kegg_files
  }else{
    d_tmp <- metacyc_files
  }
  names(d_tmp$picrust_pathway_unstrat)[grepl("L001",names(d_tmp$picrust_pathway_unstrat))] <- gsub("[A-Z]|-|(?=_).*","",names(d_tmp$picrust_pathway_unstrat)[grepl("L001",names(d_tmp$picrust_pathway_unstrat))] ,perl = T)
  
  print("% of NA")
  a <- d_tmp$picrust_pathway_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    summarise(total = sum(value))
  b <- d_tmp$picrust_pathway_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    group_by(c1,c2,c3,c4)%>%
    summarise(total = sum(value))%>%
    filter(is.na(c1))
  
  print((b$total/a$total)*100)
    
    
  plot_pth[[i]] <- d_tmp$picrust_pathway_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    mutate(across(5:ncol(.),~(.x/sum(.))*100))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c2))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values = getPalette(37))+
    ylab("Function %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))+
    theme(legend.position = "n")+
    d_tmp$picrust_pathway_unstrat%>%
    select(c(1:4,which(names(.)%in%combined$Sample)))%>%
    pivot_longer(cols =c(5:ncol(.)))%>%
    separate(col=classification,into = paste0("c",1:4),sep = ";")%>%
    ggplot(aes(x=name,y=value,fill=c2))+
    geom_bar(stat = "identity",position = "stack",width=1)+
    scale_fill_manual(values = getPalette(37))+
    ylab("Function %")+
    xlab("Samples")+
    theme_classic2()+
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank())+
    scale_y_continuous(expand = c(0, 0))
  
  ordi_pth <- d_tmp$picrust_pathway_unstrat%>%
    select(c(which(names(.)%in%combined$Sample)))%>%
    t()%>%
    metaMDS()

  df_ordi_pth_sites <- ordi_pth$points
  df_ordi_pth_sites %<>%
    as.data.frame()%>%
    mutate(Sample=rownames(.))%>%
    left_join(combined)
  
  ordiplot_pth[[i]] <- ggplot(df_ordi_pth_sites,aes(x=MDS1,y=MDS2,label = Sample,color=Country))+
    geom_point()+
    theme_dark()+
    # geom_text()+
    scale_color_manual(values=country_color)
}
```


### Xplore diversity within picrust2


